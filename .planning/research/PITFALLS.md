# Domain Pitfalls: Adding Distributed Actor Support to an Existing Single-Node Runtime

**Domain:** Adding BEAM-style distributed actor system (multi-node clustering, location-transparent PIDs, remote spawn, remote monitoring, binary wire format, node registry) to an existing single-node actor runtime
**Researched:** 2026-02-12
**Confidence:** HIGH (based on direct Snow codebase analysis of process.rs, scheduler.rs, mailbox.rs, actor/mod.rs, heap.rs, link.rs, registry.rs, gc.rs, service.rs, intrinsics.rs; Erlang/OTP distribution protocol documentation; BEAM external term format specification; academic research on distributed actor GC; real-world pitfalls from Erlang, Akka, Proto.Actor, Swift distributed actors)

**Scope:** This document covers pitfalls specific to adding distribution to Snow's existing single-node actor runtime. Each pitfall is analyzed against Snow's current architecture: ProcessId as `pub struct ProcessId(pub u64)` with sequential AtomicU64 counter, M:N work-stealing scheduler with corosensei coroutines (!Send, thread-pinned), per-actor mark-sweep GC with conservative stack scanning, MessageBuffer with type_tag + raw bytes for cross-heap message copying, FIFO mailbox with Mutex<VecDeque<Message>>, bidirectional process linking with EXIT_SIGNAL_TAG, named process registry with String-to-ProcessId mapping, and extern "C" ABI functions returning PIDs as `u64`.

**Zero-regressions constraint:** Snow has 1,524 passing tests and a zero-regressions policy. Every pitfall must be evaluated for backward compatibility with existing single-node programs.

---

## Critical Pitfalls

Mistakes that cause backward compatibility breaks, data corruption across nodes, silent message loss, or require rewrites of core runtime components.

---

### Pitfall 1: PID Representation Change Breaks the Entire ABI Surface

**What goes wrong:**
Snow's ProcessId is currently a bare `u64` (`pub struct ProcessId(pub u64)`, process.rs line 24). PIDs are generated by `AtomicU64::fetch_add(1, Ordering::Relaxed)` (process.rs line 29), monotonically incrementing from 0. The entire extern "C" ABI passes PIDs as raw `u64` values: `snow_actor_spawn` returns `u64`, `snow_actor_send` takes `target_pid: u64`, `snow_actor_self` returns `u64`, `snow_actor_link` takes `u64`, `snow_supervisor_start` returns `u64`, `snow_service_call` takes `target_pid: u64`. The LLVM codegen (intrinsics.rs) declares all PID parameters as `i64_type`. The type system represents `Pid<M>` as an opaque u64 at the IR level.

For distribution, a PID must encode: (a) which node the process lives on, (b) a process-local identifier, and (c) a creation counter to distinguish node incarnations (so a PID from a crashed-and-restarted node is not confused with a PID from the new incarnation). Erlang's NEW_PID_EXT uses: Node (atom), ID (32-bit), Serial (32-bit), Creation (32-bit) -- far more than 64 bits. Even a minimal scheme (16-bit node ID + 48-bit process ID) changes the PID semantics.

If the PID representation changes from a plain u64 to a wider struct or encoded value, EVERY extern "C" function signature changes, EVERY codegen call site changes, EVERY test that constructs or compares PIDs changes. This is a project-wide ABI break.

**Why it happens:**
- The u64 PID was a perfectly reasonable design for single-node: it is simple, cheap, and fits in a register
- Distribution was not a design constraint when the PID system was created
- The u64 representation is deeply embedded: it is the ABI, the type system representation, the process table key, the links HashSet element, the registry value, the exit signal encoding, and the codegen IR
- Erlang's evolution shows the same pattern: PID representation expanded multiple times (15-bit to 28-bit, then 32-bit ID + 32-bit Serial + 32-bit Creation)

**Consequences:**
- Changing PID to a wider type (e.g., 128-bit struct) breaks all 1,524 tests
- Every extern "C" function signature must change
- All LLVM IR that manipulates PIDs must be regenerated
- Compiled Snow programs from before the change are binary-incompatible
- The ProcessId Hash/Eq/Ord implementations change, affecting process table lookups

**Prevention:**
1. **Keep ProcessId as u64 internally but encode node information within the 64 bits.** Use a bit-packing scheme: e.g., upper 16 bits = node ID (0 = local), next 8 bits = creation counter, lower 40 bits = process index. This gives 65,535 nodes, 256 incarnations per node, and ~1 trillion process IDs per node. The extern "C" ABI stays u64 -- no signature changes.
2. **Reserve node ID 0 to mean "local node."** All existing PIDs have node ID 0. Existing programs produce and consume PIDs with node ID 0 and work exactly as before. The PID counter (`AtomicU64::fetch_add`) is adjusted to only increment within the lower 40 bits.
3. **Add extraction functions** (`ProcessId::node_id()`, `ProcessId::local_index()`, `ProcessId::creation()`) that decode the bit fields. These are used by the distribution layer but invisible to existing single-node code.
4. **Validate backward compatibility:** Existing code that does `ProcessId(some_u64)` or `pid.as_u64()` continues to work because the representation is still u64. The only semantic change is that PIDs from remote nodes have nonzero upper bits.
5. **Write a migration test:** Run all 1,524 existing tests with the new bit-packed PID scheme (all local PIDs have node_id=0). Zero regressions means the encoding is compatible.

**Detection:**
- Test failures in ANY existing test after PID representation change
- LLVM codegen errors about type mismatches (i64 vs struct)
- Process table lookup failures (Hash/Eq changes)
- PID display format changes (currently `<0.N>` -- must still render correctly for local PIDs)

**Phase:** PID representation. This must be the FIRST thing addressed because everything else depends on it. The bit-packing scheme must be designed and validated before any distribution code is written.

---

### Pitfall 2: Message Serialization Cannot Handle Arbitrary Snow Values

**What goes wrong:**
Snow's current message passing works by deep-copying raw bytes between actor heaps. `snow_actor_send` (actor/mod.rs line 261) receives a `msg_ptr` and `msg_size`, copies the bytes into a `MessageBuffer`, and delivers them to the target mailbox. The receiver copies them into its own heap via `copy_msg_to_actor_heap` (actor/mod.rs line 484). This works for local messaging because both sender and receiver share the same address space -- raw memory layouts (including pointers to heap-allocated strings, lists, maps, closures) are valid in both heaps.

For remote messaging, raw byte copying is catastrophically wrong. The raw bytes contain:
- Pointers to heap-allocated objects (strings, lists, maps) that are only valid in the sender's address space
- Closure environments with captured function pointers that reference specific code addresses in the sender's compiled binary
- Collection handles (lists, maps, sets) that are opaque Rust Box/Arc pointers
- Type tags derived from the first 8 bytes of the message (actor/mod.rs lines 274-279), which may not be meaningful to the remote node

If these raw bytes are sent over the wire to another node, the pointers dereference garbage, closure calls jump to random code, and collection handles crash the receiver.

**Why it happens:**
- Local message passing intentionally copies raw bytes for performance -- it avoids serialization overhead
- Snow values at runtime are a mix of immediates (i64 for Int, Bool, Float via bit punning) and heap pointers (strings, collections, ADTs)
- There is no serialization/deserialization layer because single-node messaging never needed one
- The codegen emits messages as flat byte arrays containing the direct memory representation of Snow values

**Consequences:**
- Sending any message containing a String, List, Map, Set, Tuple, or closure to a remote node causes segfault or data corruption on the receiver
- Even messages containing only integers and booleans may fail if the type_tag derivation produces incompatible values across nodes
- The problem is not just technical but architectural: there is no "serialize a Snow value" function anywhere in the codebase

**Prevention:**
1. **Build a Snow value serialization format (External Term Format).** Define a binary encoding for every Snow type: Int (tag + 8 bytes), Float (tag + 8 bytes), Bool (tag + 1 byte), String (tag + length + UTF-8 bytes), Atom (tag + length + bytes), List (tag + length + serialized elements), Map (tag + count + serialized key-value pairs), Tuple (tag + arity + serialized elements), Pid (tag + u64 bit-packed PID), ADT variant (tag + variant ID + serialized fields). This format must be self-describing so the receiver can reconstruct values without knowing the schema.
2. **Closures and function pointers are NOT serializable across nodes.** This is a fundamental limitation shared by Erlang/BEAM, Akka, and virtually all distributed actor systems. Detect attempts to send closures remotely and return an error. (Erlang allows remote funs only when the same module is loaded on both nodes -- defer this complexity.)
3. **Keep local messaging on the fast path.** When sender and receiver are on the same node (node_id in PID == local node_id), use the existing raw byte copy. Only invoke serialization when the target PID has a different node_id. This preserves zero performance regression for local-only programs.
4. **The serialization format must handle recursive/nested structures.** A List of Maps of Strings requires recursive serialization. Implement this iteratively with an explicit stack to avoid Rust stack overflow on deeply nested values.
5. **Version the wire format.** Include a version byte at the start of every serialized message. This allows future format changes without breaking running clusters.

**Detection:**
- Segfaults on the receiving node when deserializing messages containing heap pointers
- Garbled string data (the pointer value is interpreted as string bytes)
- Test: send a `{:hello, "world", [1, 2, 3]}` message to a remote node and verify it arrives intact
- Any message containing a closure sent remotely should produce a clear error, not a crash

**Phase:** Wire format / serialization. Must be designed before implementing any remote send. The serialization format is the single most complex piece of the distribution layer.

---

### Pitfall 3: Network Partition Causes Split-Brain in Named Process Registry

**What goes wrong:**
Snow's `ProcessRegistry` (registry.rs) is a global in-memory `FxHashMap<String, ProcessId>` with an `RwLock`. It supports `register(name, pid)`, `whereis(name)`, and `cleanup_process(pid)`. Names are unique: registering an already-taken name returns `NameAlreadyRegistered`.

In a distributed system, named process registration must be cluster-wide: if node A registers `:my_server`, node B calling `whereis(:my_server)` should find the process on node A. During a network partition, nodes A and B cannot communicate. If both sides allow registering `:my_server` locally, the cluster has two processes with the same name -- a split-brain scenario. When the partition heals, the registry is inconsistent.

Erlang's `global` module attempts to solve this with a distributed lock protocol, but it is notoriously problematic. The Erlang documentation states: "Recovery from Communication Failure" requires manual intervention to pick which side is correct. Mnesia's split-brain handling explicitly acknowledges that data loss is possible.

**Why it happens:**
- The current registry is purely local -- there is no concept of "cluster-wide registration"
- Network partitions are an unavoidable reality in distributed systems (the "P" in CAP theorem)
- Providing strong consistency for naming requires distributed consensus (Paxos/Raft), which is complex and adds latency to every registration
- Providing availability means allowing local registrations during partitions, risking duplicates

**Consequences:**
- Two processes on different nodes with the same registered name
- Messages sent to the name on one side go to a different process than on the other side
- After partition heals, cleanup is ambiguous: which registration wins?
- Supervisors that rely on named processes for service discovery may restart services incorrectly

**Prevention:**
1. **Start with LOCAL-ONLY named registration, explicitly scoped by node.** `register(:my_server)` registers on the local node only. `whereis(:my_server)` looks up only the local registry. To address a named process on a remote node, use `{:my_server, :node_b@host}` syntax. This is exactly Erlang's default behavior and avoids the split-brain problem entirely.
2. **Defer global (cluster-wide) registration to a later phase.** Global registration requires a distributed consensus protocol (or an eventually-consistent design with conflict resolution). This is a separate, complex subsystem.
3. **Document the scoping clearly.** Users must understand that `register/whereis` are local-node operations. Remote named sends require explicit node qualification.
4. **When implementing global registration later, use a leader-based approach:** one node is elected as the "name server" (like Erlang's `global` module), with failover. Accept that during partitions, global name resolution is unavailable rather than inconsistent.

**Detection:**
- Two processes on different nodes respond to the same registered name
- Messages sent to a registered name arrive at an unexpected process after partition heal
- Service discovery returns stale PIDs pointing to dead processes on partitioned nodes

**Phase:** Global process registry. Keep local-only in the initial distribution phase. Defer global naming to a dedicated phase with explicit split-brain strategy.

---

### Pitfall 4: GC Collects Objects Referenced by In-Flight Remote Messages

**What goes wrong:**
Snow's per-actor GC uses conservative stack scanning to find roots, then traces from those roots through the heap (heap.rs `mark_from_roots`). The GC only knows about objects reachable from the current actor's stack. When an actor sends a message to a remote node, the message bytes are serialized and placed in an OS-level send buffer (or a runtime queue for TCP transmission). The original Snow values that were serialized may no longer be referenced from the sending actor's stack -- the actor has moved on to other work. If GC runs on the sending actor before the serialization is complete (or while the serialized bytes are being written to the wire), the source objects may be collected.

More critically, the receiving side: when a remote message arrives and is deserialized into Snow values on the receiving actor's heap, those values exist only in the actor's heap. If the receiving actor hasn't yet stored references to the deserialized values on its stack (e.g., the values are in an intermediate buffer), a GC cycle could sweep them before the actor processes them.

For remote references (a PID on node A referencing a process on node B): if node B's process exits and its heap is reclaimed, but node A still holds the PID and tries to send a message, the message delivery fails. This is expected and handled (the send silently drops). But if node A holds a reference to a value that was allocated on node B's heap (which should never happen with proper serialization), the reference is dangling.

**Why it happens:**
- Per-actor GC has no knowledge of in-flight network messages
- Conservative stack scanning only finds roots on the stack; objects reachable only from a network buffer or OS send queue are invisible
- The GC runs at yield points (`try_trigger_gc` in actor/mod.rs line 205), which can happen between serializing a message and confirming it was sent
- Remote message deserialization creates new heap objects that may not yet be rooted on the stack

**Consequences:**
- Serialized messages reference freed memory, causing corrupted data on the wire (if serialization reads from collected objects)
- Deserialized values are collected before the actor processes them, causing crashes or logic errors
- Intermittent, timing-dependent bugs that only manifest under GC pressure during network I/O

**Prevention:**
1. **Serialize before releasing references.** When sending a remote message, serialize the Snow values into a self-contained byte buffer (Vec<u8> on the Rust heap, NOT on the actor heap) BEFORE allowing the actor to continue. The serialization captures all values by copy. After serialization, the original objects on the actor heap can be safely collected -- the byte buffer is independent.
2. **Use Rust heap (Vec, Box) for serialization buffers, NOT the actor GC heap.** The actor heap is subject to mark-sweep collection. The Rust heap (System allocator) is not. Serialization buffers live on the Rust heap until the TCP write completes, then are dropped via normal Rust ownership.
3. **On the receiving side, deserialize directly into the receiving actor's heap.** The deserialization function allocates new objects on the receiving actor's `ActorHeap` via `heap.alloc()`. These objects are immediately rooted because deserialization happens inside the actor's execution context (on the coroutine stack), so conservative stack scanning will find references to them.
4. **Never share heap objects across nodes.** All cross-node communication goes through serialization/deserialization. There are no "remote heap references." Each node's GC operates independently, which is exactly how Erlang works.
5. **Pin serialization buffers during TCP write.** If the TCP write is asynchronous (write buffer + yield + resume), ensure the serialization buffer is not dropped or moved between the write call and the completion. Using a Vec owned by the connection actor's state (not the sending actor's heap) ensures this.

**Detection:**
- Corrupted messages arriving at remote nodes (wrong values, truncated data)
- Segfaults during serialization when GC has collected the source object between "start serialize" and "read field"
- Intermittent: only manifests when GC triggers during message serialization, which depends on heap pressure
- Test: send large messages under heavy allocation pressure (force frequent GC cycles)

**Phase:** Serialization + GC integration. Must be addressed in the serialization design phase. The key insight is: serialize to Rust heap (immune to GC), not actor heap.

---

### Pitfall 5: Local Messaging Performance Regresses When Distribution Code Is Added

**What goes wrong:**
The current `snow_actor_send` (actor/mod.rs line 261) is a fast path: look up PID in process table, deep-copy bytes into mailbox, wake if Waiting. This takes microseconds. If distribution adds a check on every send ("is this PID local or remote?"), followed by conditional serialization, the overhead impacts EVERY message send, even between two actors on the same node.

Erlang learned this lesson: serialization can consume up to 70% of CPU cycles under high traffic. Even the check itself (extracting node_id from PID, comparing to local node_id) adds branch prediction pressure and instruction cache pollution to the hot path.

Snow's zero-regressions policy means that existing programs (100% local messaging) must perform identically after distribution code is added. Any measurable regression in local message send latency is a policy violation.

**Why it happens:**
- The natural implementation adds an `if is_remote(pid) { serialize_and_send_remote() } else { local_send() }` branch to every send
- Even a single branch instruction on the hot path can cause measurable regression at high message rates
- If the serialization code is linked into `snow_actor_send` (even behind a branch), it increases the function's code size, potentially causing instruction cache misses
- Lock contention can increase if distribution adds new shared state (node table, connection pool) that is accessed during PID checks

**Consequences:**
- Existing benchmark programs (actor ping-pong, supervisor restart cycles) show measurable slowdown
- Test suite execution time increases
- Users upgrading to the distributed-capable runtime see degraded single-node performance

**Prevention:**
1. **Use the PID bit-packing scheme (Pitfall 1) for a zero-cost local check.** If node_id is in the upper 16 bits of the u64 PID, checking for local is `pid.0 >> 48 == 0` or `pid.0 & NODE_MASK == LOCAL_NODE`. This is a single bitwise AND + compare -- essentially free on modern CPUs.
2. **Keep the local send path COMPLETELY unchanged.** The check goes at the very top of `snow_actor_send`: `if pid is local { existing code path unchanged } else { remote_send(pid, msg) }`. The else branch is a separate function (not inlined) to keep the hot path's instruction footprint minimal.
3. **Put remote send logic in a separate module/function** that is NOT compiled into the hot path. Use `#[cold]` attribute on the remote path and `#[inline(always)]` on the local check.
4. **Benchmark before and after.** Create a micro-benchmark: 1 million local sends between two actors, measure mean latency. This benchmark must show no regression (within noise floor) after distribution code is added.
5. **The remote send path can be lazy-loaded.** If no distribution is configured (single-node program), the remote path is never called and its code may not even be paged into memory.

**Detection:**
- Micro-benchmark: local actor ping-pong latency before vs. after
- Test suite execution time comparison
- CPU profiling shows new instructions in the `snow_actor_send` hot path
- Cache miss counters increase on the hot path

**Phase:** Integration point for remote send. Must be benchmarked during implementation. Design the local/remote branch as the very first thing in the distribution integration.

---

### Pitfall 6: PID Reuse After Node Restart Causes Messages Sent to Wrong Process

**What goes wrong:**
When a node crashes and restarts, its PID counter resets to 0. New processes on the restarted node receive the same PID values (within the local index bits) as processes on the previous incarnation. If another node cached a PID from the old incarnation and sends a message to it, the message is delivered to a completely different process on the new incarnation.

This is a well-documented Erlang pitfall. From the "Programming Distributed Erlang Applications: Pitfalls and Recipes" paper: "the new process executing echo at N2 was spawned with the same process identifier as a process spawned on an earlier instance of N2. Thus in a distributed protocol one likely has to resort to developing and using another mechanism to accurately identify communicating partners."

Erlang solves this with the "Creation" counter: each node incarnation gets a unique creation number (assigned by EPMD). PIDs include the creation number, so a PID from incarnation 5 is distinct from a PID from incarnation 6, even if the process index is the same.

**Why it happens:**
- Snow's PID counter (`AtomicU64::fetch_add(1, Ordering::Relaxed)`, process.rs line 29) starts at 0 on every runtime startup
- Without a creation counter, there is no way to distinguish "PID 42 from node B version 1" from "PID 42 from node B version 2"
- Remote nodes may cache PIDs for long periods (e.g., a supervisor on node A holds the PID of a worker on node B)
- The problem is invisible until a node restarts and old PIDs are reused

**Consequences:**
- Messages intended for a dead process are delivered to an unrelated new process
- Exit signals from a dead process arrive at a new process, causing unexpected termination
- Supervisors restart wrong processes or fail to detect that a monitored process has been replaced
- Data corruption: a GenServer call response is delivered to a different caller
- Extremely hard to debug: behavior appears correct most of the time, fails only after node restarts

**Prevention:**
1. **Include a creation counter in the PID bit-packing.** As proposed in Pitfall 1: upper 16 bits = node ID, next 8 bits = creation counter, lower 40 bits = process index. The 8-bit creation counter gives 256 incarnations before wrapping, which is sufficient for most use cases.
2. **When a node joins the cluster, assign it a creation counter** that is incremented from the previous value known by the cluster. Store this in the node registry.
3. **When receiving a message for a local PID, validate the creation counter.** If the creation counter in the PID does not match the current node's creation, silently drop the message (the target process no longer exists in this incarnation). This is exactly how Erlang handles it.
4. **When a node detects that a remote node has restarted (new creation counter), invalidate all cached PIDs** from the old incarnation. Send exit signals to local processes that were linked to/monitoring processes on the restarted node.

**Detection:**
- Messages delivered to wrong process after node restart (test by: start node B, create process P1, send PID to node A, kill and restart node B, create process P2, observe if A's message arrives at P2)
- Linked processes on node A do NOT receive exit signals when node B restarts (they should receive `{:exit, pid, :noconnection}`)
- Monitor messages reference the wrong process after node restart

**Phase:** PID design / node lifecycle. The creation counter must be part of the PID bit-packing design (Pitfall 1). It cannot be retrofitted.

---

### Pitfall 7: Links and Monitors Do Not Propagate Across Node Boundaries

**What goes wrong:**
Snow's linking system (link.rs) stores links as `links: HashSet<ProcessId>` in each Process struct. When a process exits, `propagate_exit` (link.rs line 178) iterates over the linked PIDs and delivers exit signals by looking up each PID in the local process table (`process_table.read().get(&linked_pid)`). If the linked PID is on a remote node, the local process table lookup returns None, and the exit signal is silently lost.

This means: if actor A on node 1 links to actor B on node 2, and actor B crashes, actor A never finds out. The link is broken silently. Supervisors that supervise remote children will not detect child crashes. This undermines the entire fault-tolerance model.

**Why it happens:**
- `propagate_exit` uses a closure `get_process: F where F: Fn(ProcessId) -> Option<Arc<Mutex<Process>>>` that only queries the LOCAL process table
- There is no concept of a remote process table or a remote exit signal delivery
- Links are stored as plain ProcessIds in a HashSet -- there is no distinction between local and remote links
- The exit signal encoding (link.rs `encode_exit_signal`) produces raw bytes that could be serialized, but the delivery mechanism does not support remote delivery

**Consequences:**
- Remote links are silently broken: linking to a remote process gives the illusion of safety but provides none
- Remote supervision is impossible: the supervisor never receives exit signals from remote children
- `trap_exit` on a supervisor trapping remote child exits never sees the exit message
- The "let it crash" philosophy breaks across node boundaries

**Prevention:**
1. **When a process exits and has links to remote PIDs, send the exit signal to the remote node.** The distribution layer must intercept `propagate_exit` and, for each linked PID with a non-local node_id, send the exit signal over the network to the remote node.
2. **The remote node's distribution handler receives the exit signal and delivers it locally** to the linked process, using the same `propagate_exit` logic (trap_exit check, Waiting wake, etc.).
3. **When a node connection drops (detected by TCP close or heartbeat timeout), generate `:noconnection` exit signals** for ALL local processes that are linked to processes on the disconnected node. This is how Erlang handles it: a net_tick timeout triggers exit signals for all linked remote processes.
4. **Implement monitors (in addition to links) for remote processes.** Monitors are unidirectional (A monitors B; if B dies, A gets a `:DOWN` message). This is more appropriate for distributed supervision because it does not require the remote process to know about the monitor.
5. **Store remote links separately** or tag them in the links HashSet so that the exit propagation code knows to use the network path.

**Detection:**
- Link actor A to remote actor B, crash B, verify A receives exit signal (this will FAIL without the fix)
- Supervisor on node 1 supervising child on node 2: kill child, verify supervisor restarts it (this will FAIL)
- Kill network connection between nodes: verify linked processes receive `:noconnection` exit signals

**Phase:** Remote links/monitors. This is a core distribution feature that must be implemented alongside remote messaging. Without it, distribution is unsafe.

---

## Moderate Pitfalls

Mistakes that cause incorrect behavior, poor performance, or security issues but do not require full rewrites.

---

### Pitfall 8: Erlang-Style Connection Mesh Creates O(N^2) TCP Connections

**What goes wrong:**
In Erlang's distribution model, every node maintains a direct TCP connection to every other node (fully connected mesh). When node A connects to node B, and node B is already connected to node C, node A automatically connects to node C as well (transitive connections). With N nodes, this creates N*(N-1)/2 TCP connections.

For a 100-node cluster, this means 4,950 TCP connections per node (99 outgoing). Each connection requires: a TCP socket (file descriptor), a TLS session (if encrypted, ~50KB RAM each with rustls), a heartbeat timer, and a dedicated connection handler actor. At 100 nodes with TLS, each node needs ~5MB just for TLS sessions, plus 99 file descriptors.

**Why it happens:**
- Fully connected mesh is the simplest distribution topology -- every node can send directly to every other node
- Erlang's model works well for small clusters (3-10 nodes) but degrades for larger clusters
- The transitive connection behavior is automatic and hard to opt out of

**Consequences:**
- File descriptor exhaustion (default Linux ulimit is 1024, need ~2*N for N nodes plus application FDs)
- Memory usage for connection state grows quadratically
- Heartbeat traffic grows quadratically (each connection sends/receives periodic ticks)
- Cluster startup involves a storm of N^2 TCP handshakes + TLS handshakes

**Prevention:**
1. **Start with explicit connections, not transitive auto-connect.** Nodes connect to explicitly configured peers, not to every node in the cluster. This gives users control over the topology.
2. **Support a configurable connection limit per node.** Default to direct connections but allow relay/proxy for large clusters.
3. **For the initial implementation, target clusters of 3-20 nodes.** This is sufficient for fault-tolerance use cases (multi-machine redundancy) and avoids the O(N^2) problem.
4. **Defer automatic topology management (mesh gossip, etc.) to a later phase.** Keep the initial distribution simple: manual connection with `Node.connect(:other_node)`.
5. **Monitor file descriptor usage and warn when approaching limits.** A runtime diagnostic function can report connection count and FD usage.

**Detection:**
- File descriptor exhaustion errors when connecting many nodes
- Slow cluster startup due to connection storm
- Memory growth proportional to node count squared

**Phase:** Node connection management. Design the connection topology early. Starting with explicit connections avoids the mesh problem.

---

### Pitfall 9: Wire Format Lacks Versioning, Making Rolling Upgrades Impossible

**What goes wrong:**
If the binary wire format for serialized messages does not include a version number, there is no way to evolve it. When a new Snow release changes the serialization format (adding a new type, changing encoding for an existing type, optimizing binary layout), nodes running different versions cannot communicate. In a production cluster, rolling upgrades (upgrading one node at a time) are impossible because the first upgraded node speaks a different protocol than the remaining nodes.

Erlang's external term format includes a version byte (131 = current version) and has evolved multiple times (PID_EXT to NEW_PID_EXT, REFERENCE_EXT to NEWER_REFERENCE_EXT). The version byte allows the receiver to select the correct decoder.

**Why it happens:**
- Version bytes seem like unnecessary overhead when the format is new ("we can add it later")
- The format evolves more frequently than expected in early development
- Without versioning, any format change is a cluster-wide breaking change requiring simultaneous restart of all nodes

**Consequences:**
- Cluster-wide restart required for any wire format change
- Deserialization crashes or silent data corruption when nodes running different versions communicate
- No ability to add new types or optimize encoding without breaking compatibility

**Prevention:**
1. **Include a version byte as the first byte of every serialized message.** Version 1 for the initial format.
2. **Include a version byte in the distribution handshake.** When two nodes connect, they exchange supported protocol versions and agree on the highest common version.
3. **Maintain backward-compatible deserialization for at least one previous version.** New code can deserialize old format; old code rejects new format with a clear error.
4. **Reserve type tags for future use.** Leave gaps in the type tag numbering so new types can be added without renumbering.

**Detection:**
- Deserialization errors when connecting nodes with different Snow versions
- Silent data corruption (wrong type interpretation) if format changes without versioning

**Phase:** Wire format design. The version byte is a day-one requirement. Adding it later requires a breaking change.

---

### Pitfall 10: Supervisor Strategies Assume Instant Process Restart, Fail with Network Latency

**What goes wrong:**
Snow's supervisor restart logic (supervisor.rs) is designed for local restarts: spawn a new process, link it, update the child state. This is near-instantaneous (microseconds). With distributed supervision (supervisor on node A, children on node B), restarting a child requires: network round-trip to node B, remote spawn, waiting for confirmation, updating local state. This takes milliseconds to tens of milliseconds.

During this time, the supervisor's `max_restarts` / `max_seconds` rate limiter may trigger incorrectly. If the supervisor has `max_restarts: 3, max_seconds: 5` and three children crash simultaneously on a remote node, the restarts may take 3 * 100ms = 300ms, but the supervisor's timer counts them as happening in the same window. If network latency causes the restarts to appear as a burst, the supervisor may exceed its restart limit and shut down the entire supervision tree.

**Why it happens:**
- Restart timing assumptions are based on local spawn latency
- Network latency adds 1-100ms per restart, which is 1000x slower than local
- Batch failures (node crash) cause all children to fail simultaneously, but restarts must happen sequentially
- The `max_restarts / max_seconds` window does not account for restart latency

**Consequences:**
- Supervisors shut down prematurely during remote node recovery
- One-for-all and rest-for-one strategies are particularly vulnerable: a single child crash triggers restart of all/subsequent children, each with network latency
- The supervision tree becomes less fault-tolerant with distribution, not more

**Prevention:**
1. **Separate "restart initiated" from "restart completed" in the rate limiter.** Count restart initiations (which are fast) rather than completions (which are slow).
2. **For remote children, increase the default `max_restarts` / `max_seconds` window** or document that users should tune these values for distributed deployments.
3. **Implement asynchronous restart: initiate the remote spawn and continue processing** other exit signals. When the remote spawn confirmation arrives, update the child state. This prevents restart serialization.
4. **Handle `:noconnection` exits differently from child crashes.** If a child exits because the node went down (`:noconnection`), the supervisor should not count this as a restart against the rate limit -- it should wait for the node to come back and re-spawn.

**Detection:**
- Supervisor shuts down during remote node recovery even though fewer than `max_restarts` children failed
- Log messages showing "max restart intensity reached" during network-related restarts
- Supervision tree collapse after transient network partition

**Phase:** Distributed supervision. Address when implementing remote supervisor-child relationships. Local supervision is unaffected.

---

### Pitfall 11: Node Connection Authentication Is Skipped, Allowing Arbitrary Remote Code Execution

**What goes wrong:**
In Erlang's distribution protocol, nodes authenticate using a shared "cookie" -- a secret string that must match between connecting nodes. Without authentication, any machine that can reach the EPMD port can connect to the cluster, send spawn requests, and execute arbitrary code on any node. This is a critical security vulnerability.

Snow already has TLS infrastructure (rustls 0.23), which provides transport encryption and can provide mutual authentication via client certificates. But if distribution connections are implemented without any authentication (just plain TCP or even TLS without client cert verification), any machine can join the cluster.

**Why it happens:**
- Authentication is easy to defer ("we will add it later")
- TLS provides encryption but not necessarily authorization (a valid TLS connection from an untrusted client is still dangerous)
- Erlang's cookie-based auth is weak (shared secret, no rotation), but it is better than nothing
- Without auth, testing is easier (no cert management)

**Consequences:**
- Remote code execution: a malicious node can spawn actors on any cluster node
- Data exfiltration: a malicious node can receive messages intended for legitimate actors
- Cluster disruption: a malicious node can send exit signals to arbitrary processes

**Prevention:**
1. **Require mutual TLS (mTLS) for all distribution connections.** Each node has a TLS certificate signed by a shared CA. Connecting nodes present their certificate; the receiving node verifies it against the CA. Snow already uses rustls 0.23, which supports mTLS.
2. **As a simpler alternative (for development), implement cookie-based authentication.** Exchange a shared secret during the distribution handshake. This is insecure against network sniffing but prevents unauthorized connections.
3. **Support both modes:** cookie-only (for development/testing), TLS-only (for production), mTLS (for high-security). Default to requiring at least cookie auth.
4. **Never allow unauthenticated distribution connections, even in development.** The default should be secure.

**Detection:**
- Arbitrary machines can connect to a node and execute spawn requests
- Security audit reveals no authentication step in the distribution handshake

**Phase:** Distribution handshake protocol. Authentication must be part of the handshake design, not bolted on later.

---

### Pitfall 12: Attempting to Send Function Pointers / Closures Over the Wire

**What goes wrong:**
Snow supports closures with captured environments. A closure at runtime is a function pointer (code address) plus an environment pointer (captured variables on the heap). If a message containing a closure is sent to a remote node, the function pointer is meaningless on the remote node (different address space, potentially different binary) and the environment pointer is dangling.

This is the most common "it works locally, crashes remotely" bug in distributed actor systems. In Erlang, sending a fun to a remote node works only if the same module version is loaded on both nodes. In Akka, serialization of closures is explicitly discouraged and requires special serializers. In most systems, it simply does not work.

**Why it happens:**
- Closures look like regular values at the Snow language level -- there is no syntactic difference between sending an integer and sending a closure
- The type system may allow `send(pid, fn(x) -> x + 1)` without error
- Function pointers are valid memory addresses that serialize as u64 values but are meaningless on another machine

**Consequences:**
- Segfault on the receiving node when it attempts to call the deserialized "function pointer" (jumping to a random code address)
- Silent wrong behavior if the address happens to be valid code on the remote node (extremely unlikely but catastrophically confusing)
- Users expect closures to work because they work locally

**Prevention:**
1. **Detect closures during serialization and return an error.** The serializer must check the type tag of each value being serialized. If it encounters a closure/function value, return `Err(CannotSerializeClosure)` with a clear error message.
2. **At the type system level, consider making `Pid<M>` distinguish local and remote.** A remote-capable Pid could have type constraints that prevent sending non-serializable types. This is advanced and may be deferred, but the serialization-level check is mandatory.
3. **Provide alternatives:** Instead of sending closures, send data that the remote node can use to look up a locally-defined function. For example, send an atom `:sort_ascending` that the remote handler pattern-matches to select a local sort function.
4. **Document this limitation prominently.** Users coming from Erlang expect funs to work remotely (they partially do in Erlang). Snow should be explicit that closures are local-only.

**Detection:**
- Segfault on remote node when processing a message containing a closure
- Error at serialization time (with the prevention above): "cannot send closure to remote node"
- Test: attempt to send `fn(x) -> x * 2` to a remote actor, verify error

**Phase:** Serialization implementation. The closure detection must be part of the serializer from day one.

---

## Minor Pitfalls

Mistakes that cause suboptimal behavior or edge-case bugs but are easy to fix.

---

### Pitfall 13: Distribution Heartbeat Interferes with Actor Scheduler Timing

**What goes wrong:**
The distribution layer needs heartbeat messages between connected nodes to detect failures (Erlang calls these "net ticks," default interval 60 seconds, response timeout 15 seconds). If heartbeat handling runs on the actor scheduler (as a regular actor), it competes with user actors for scheduling time. Under heavy load, heartbeat actors may be delayed past the timeout, causing false disconnection detection.

**Prevention:**
1. **Run distribution heartbeat on a dedicated OS thread, NOT on the actor scheduler.** The heartbeat thread sends/receives tick messages on the TCP connection independent of actor scheduling pressure.
2. **Use a generous default tick interval** (e.g., 60 seconds with 15 second timeout, matching Erlang defaults). This tolerates scheduler delays without false positives.
3. **Make tick interval configurable** for environments with different latency characteristics.

**Phase:** Node connection management. Design heartbeat as a dedicated thread from the start.

---

### Pitfall 14: Remote Process Table Lookup Adds Latency to Every `get_process` Call

**What goes wrong:**
The current `Scheduler::get_process(pid)` (scheduler.rs line 326) does a simple HashMap lookup. If distribution adds a "check if remote, if so query remote node" path to `get_process`, every local process lookup pays the cost of the remote check.

**Prevention:**
1. **Never change `get_process` to handle remote PIDs.** Remote PIDs should not be in the local process table at all. The distribution layer handles remote PIDs at a higher level (in `snow_actor_send`, before reaching `get_process`).
2. **The process table only contains local processes.** Remote PID resolution happens in the distribution connection layer, not in the scheduler.

**Phase:** Distribution integration. A design constraint, not an implementation task.

---

### Pitfall 15: Large Messages Block the Distribution Connection for All Actors

**What goes wrong:**
If a single actor sends a very large message (e.g., 100MB binary) to a remote node, the serialized bytes must be written to the TCP connection. While this large write is in progress, no other messages can be sent on the same connection (TCP is ordered). All other actors waiting to send messages to that remote node are blocked.

Erlang solved this in OTP-22 with message fragmentation: large messages are split into smaller fragments, allowing small messages to be interleaved.

**Prevention:**
1. **Implement message fragmentation for large messages** (e.g., > 64KB). Split serialized messages into fixed-size chunks with sequence numbers.
2. **Alternatively, set a maximum message size** and reject messages above the limit. Document the limit.
3. **For the initial implementation, a size limit (e.g., 16MB) is acceptable.** Fragmentation can be added later as an optimization.

**Phase:** Wire protocol. Can be deferred to an optimization phase if a size limit is acceptable initially.

---

### Pitfall 16: Tests Assume Single-Node Timing, Fail with Network Latency

**What goes wrong:**
Existing tests (e2e_actors.rs, e2e_supervisors.rs, e2e_concurrency_stdlib.rs) assume message delivery is near-instantaneous. Tests that send a message and immediately check for a result may fail when the target is on a remote node because network latency adds milliseconds of delay. Tests that check process state after a spawn may fail because remote spawn has not completed yet.

**Prevention:**
1. **All existing tests continue to run in single-node mode.** The 1,524 existing tests must not be modified. They test single-node behavior and should remain single-node tests.
2. **Create a separate test suite for distributed behavior.** These tests explicitly set up multi-node configurations (even if on localhost with different ports) and account for latency.
3. **In distributed tests, use message-based synchronization** (send a confirmation message, wait for it) rather than timing-based assertions (sleep 100ms, check state).
4. **Use deterministic simulation testing (DST) for complex distributed scenarios.** Mock the network layer to control message delivery timing, simulate partitions, and reproduce timing-dependent bugs.

**Detection:**
- Existing tests fail after distribution code is added (MUST NOT happen)
- Distributed tests are flaky (pass on fast machines, fail on slow ones)

**Phase:** Testing infrastructure. Set up the distributed test framework early. Existing tests are sacred.

---

## Phase-Specific Warnings

| Phase Topic | Likely Pitfall | Mitigation |
|-------------|---------------|------------|
| PID Representation | Pitfall 1 (ABI break), Pitfall 6 (PID reuse) | Bit-pack node ID + creation into u64; validate with existing test suite |
| Message Serialization | Pitfall 2 (heap pointers), Pitfall 4 (GC interaction), Pitfall 12 (closures) | External term format; serialize to Rust heap; detect closures |
| Wire Protocol | Pitfall 9 (no versioning), Pitfall 15 (large messages) | Version byte from day one; size limits initially |
| Remote Send | Pitfall 5 (local perf regression), Pitfall 14 (process table pollution) | Branch-free local check; keep process table local-only |
| Remote Links/Monitors | Pitfall 7 (silent link breakage) | Network-aware exit propagation; :noconnection signals |
| Node Registry | Pitfall 3 (split-brain naming) | Local-only naming initially; defer global naming |
| Node Connections | Pitfall 8 (O(N^2) mesh), Pitfall 11 (no auth), Pitfall 13 (heartbeat) | Explicit connections; mTLS; dedicated heartbeat thread |
| Distributed Supervision | Pitfall 10 (restart timing) | Async restart; separate restart initiation from completion |
| Testing | Pitfall 16 (timing assumptions) | Separate test suite; DST for complex scenarios |

---

## Sources

### Erlang/BEAM Distribution Protocol
- [Distribution Protocol (erts v16.2)](https://www.erlang.org/doc/apps/erts/erl_dist_protocol.html) -- canonical distribution protocol reference
- [External Term Format (erts v16.2)](https://www.erlang.org/doc/apps/erts/erl_ext_dist.html) -- NEW_PID_EXT, serialization format
- [Distributed Erlang Reference Manual](https://www.erlang.org/doc/system/distributed.html) -- distribution semantics
- [Distribunomicon (Learn You Some Erlang)](https://learnyousomeerlang.com/distribunomicon) -- practical distribution pitfalls
- [Format of Pids (erlang-questions 2003)](https://erlang.org/pipermail/erlang-questions/2003-June/009046.html) -- PID structure details
- [Erlang PID internal representation](http://wudixiaotie.github.io/erlang/2016/04/15/erlang-pid.html) -- internal vs external PID format
- [A few notes on message passing (Erlang blog)](https://www.erlang.org/blog/message-passing/) -- message copying, GC interaction

### Research Papers
- [Programming Distributed Erlang Applications: Pitfalls and Recipes (Svensson, Fredlund)](https://dl.acm.org/doi/10.1145/1292520.1292527) -- PID reuse, naming, semantic differences
- [Actor Garbage Collection in Distributed Systems using Graph Transformation](https://arxiv.org/abs/1201.2312) -- DGC fundamentals
- [Fault-Tolerant Reference Counting for GC in Distributed Systems](https://www.academia.edu/6212806/Fault_Tolerant_Reference_Counting_for_Garbage_Collection_in_Distributed_Systems) -- DGC with node failures
- [Concurrent garbage collection in the actor model](https://www.researchgate.net/publication/328511134_Concurrent_garbage_collection_in_the_actor_model) -- per-actor GC challenges

### Distributed Systems Testing
- [Deterministic Simulation: A New Era of Distributed System Testing](https://www.risingwave.com/blog/deterministic-simulation-a-new-era-of-distributed-system-testing/) -- DST overview
- [Testing Distributed Systems (curated list)](https://asatarin.github.io/testing-distributed-systems/) -- comprehensive resource list
- [FoundationDB's Simulation Testing](https://alex-ii.github.io/notes/2018/04/29/distributed_systems_with_deterministic_simulation.html) -- pioneering DST approach

### Network Partitions and Split-Brain
- [Mnesia and CAP (Jesper L. Andersen)](https://medium.com/@jlouis666/mnesia-and-cap-d2673a92850) -- Mnesia split-brain limitations
- [Split brain in distributed Erlang (erlang-questions)](https://erlang-questions.erlang.narkive.com/DcY4LdIO/split-brain-in-disributed-erlang) -- community discussion of strategies
- [epmdpxy (GitHub)](https://github.com/dergraf/epmdpxy) -- simulating netsplits for testing

### Actor System Distribution Patterns
- [Swift Distributed Actors (WWDC22)](https://developer.apple.com/videos/play/wwdc2022/110356/) -- typed distributed actors
- [Akka Serialization](https://doc.akka.io/libraries/akka-core/current/serialization.html) -- actor message serialization
- [Proto.Actor Clustering](https://proto.actor/docs/clusterintro/) -- virtual actor distribution
- [Actor-based Concurrency (berb.github.io)](https://berb.github.io/diploma-thesis/original/054_actors.html) -- location transparency

### Snow Codebase References
- `crates/snow-rt/src/actor/process.rs` -- ProcessId(pub u64), Process, ProcessState, ExitReason
- `crates/snow-rt/src/actor/scheduler.rs` -- M:N scheduler, ProcessTable, work-stealing
- `crates/snow-rt/src/actor/mod.rs` -- snow_actor_send, snow_actor_receive, extern "C" ABI
- `crates/snow-rt/src/actor/heap.rs` -- ActorHeap, GcHeader, MessageBuffer, mark-sweep GC
- `crates/snow-rt/src/actor/link.rs` -- bidirectional links, propagate_exit, EXIT_SIGNAL_TAG
- `crates/snow-rt/src/actor/registry.rs` -- ProcessRegistry, register/whereis/cleanup
- `crates/snow-rt/src/actor/service.rs` -- snow_service_call, snow_service_reply
- `crates/snow-rt/src/gc.rs` -- snow_gc_alloc_actor, per-actor heap allocation, conservative stack scanning
- `crates/snow-codegen/src/codegen/intrinsics.rs` -- LLVM function declarations, PID as i64_type
