---
phase: 94-multi-node-clustering
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - mesher/ingestion/pipeline.mpl
autonomous: true
gap_closure: true

must_haves:
  truths:
    - "When load exceeds threshold and peers exist, try_remote_spawn calls Node.spawn to spawn an event_processor_worker on a peer node"
    - "event_processor_worker function looks up its own node's PipelineRegistry via Process.whereis and uses the local pool for processing"
    - "load_monitor calls Node.monitor for each peer to receive NODEDOWN notifications"
    - "load_monitor logs NODEDOWN detection when peer count decreases between checks"
  artifacts:
    - path: "mesher/ingestion/pipeline.mpl"
      provides: "event_processor_worker function, Node.spawn call in try_remote_spawn, Node.monitor calls in load_monitor"
      contains: "Node.spawn"
  key_links:
    - from: "mesher/ingestion/pipeline.mpl (try_remote_spawn)"
      to: "Node.spawn"
      via: "Node.spawn(target, event_processor_worker) call in try_remote_spawn"
      pattern: "Node\\.spawn"
    - from: "mesher/ingestion/pipeline.mpl (load_monitor)"
      to: "Node.monitor"
      via: "Node.monitor(node_name) call for each peer"
      pattern: "Node\\.monitor"
    - from: "mesher/ingestion/pipeline.mpl (event_processor_worker)"
      to: "Process.whereis(mesher_registry)"
      via: "Worker looks up local registry to get pool handle"
      pattern: "Process\\.whereis"
---

<objective>
Close two verification gaps from Phase 94: (1) implement actual Node.spawn call in try_remote_spawn instead of the current stub that only logs intent, and (2) add Node.monitor calls for peer health tracking.

Purpose: Satisfies CLUSTER-05 requirement (system spawns remote processor actors on other nodes when local load is high) and adds node failure detection. These are the last two gaps blocking Phase 94 completion.
Output: Modified pipeline.mpl with working remote processor spawning and node monitoring.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/94-multi-node-clustering/94-RESEARCH.md
@.planning/phases/94-multi-node-clustering/94-03-SUMMARY.md
@.planning/phases/94-multi-node-clustering/94-VERIFICATION.md
@mesher/ingestion/pipeline.mpl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement event_processor_worker and wire Node.spawn into try_remote_spawn</name>
  <files>mesher/ingestion/pipeline.mpl</files>
  <action>
Modify `pipeline.mpl` to add a remotely-spawnable worker function and replace the try_remote_spawn stub with an actual Node.spawn call.

**Part A: Define event_processor_worker function**

Add a new function `event_processor_worker` BEFORE `try_remote_spawn` (define-before-use). This function is designed to be spawned remotely via Node.spawn. It looks up the LOCAL node's PipelineRegistry and EventProcessor, then processes events from that node's processor. It does NOT accept a PoolHandle parameter (PoolHandle is a raw pointer, meaningless on remote nodes -- research pitfall 1).

```
# Worker function for remote event processing (CLUSTER-05).
# Spawned on remote nodes via Node.spawn. Looks up THIS node's own
# PipelineRegistry via Process.whereis to get the local pool and processor.
# Does NOT accept PoolHandle as argument (raw pointer, not serializable -- pitfall 1).
fn event_processor_worker() do
  let reg_pid = Process.whereis("mesher_registry")
  let pool = PipelineRegistry.get_pool(reg_pid)
  let _ = EventProcessor.start(pool)
  println("[Mesher] Remote event processor worker started")
  0
end
```

This function:
1. Looks up the local PipelineRegistry (every node has one via Process.register)
2. Gets the local pool handle from the registry
3. Starts a new EventProcessor using the local pool
4. The spawned EventProcessor is an additional processor on that node, helping absorb load

**Part B: Replace try_remote_spawn stub with Node.spawn call**

Replace the current `try_remote_spawn` function (lines 254-261 of pipeline.mpl) which only logs intent and discards the get_pool result:

Current (STUB):
```
fn try_remote_spawn(nodes) do
  let target = List.head(nodes)
  let _ = println("[Mesher] Load high -- spawning remote processor on " <> target)
  let remote_reg = Global.whereis("mesher_registry@" <> target)
  let _ = PipelineRegistry.get_pool(remote_reg)
  let _ = println("[Mesher] Remote processor delegation available via " <> target)
  0
end
```

Replace with:
```
fn try_remote_spawn(nodes) do
  let target = List.head(nodes)
  let _ = println("[Mesher] Load high -- spawning remote processor on " <> target)
  let remote_pid = Node.spawn(target, event_processor_worker)
  let _ = println("[Mesher] Spawned remote processor (pid " <> String.from(remote_pid) <> ") on " <> target)
  0
end
```

Key design points:
- `Node.spawn(target, event_processor_worker)` takes the target node name (String) and a function reference. The codegen extracts the function name as a string and sends it to the remote node's FUNCTION_REGISTRY. No arguments are passed because the worker looks up its own node's registry.
- The return value is the remote PID (Int, u64). We log it for diagnostics.
- No PoolHandle is sent across nodes. The remote worker uses its own node's pool.
- `Node.spawn` returns 0 on failure (not connected, function not found, etc.) which is harmless -- the log will show pid 0 indicating the spawn didn't succeed.

Important constraints:
- `event_processor_worker` must be defined BEFORE `try_remote_spawn` (Mesh define-before-use rule)
- `event_processor_worker` takes no arguments (zero-arg function) -- this avoids PoolHandle serialization issues entirely
- Both functions must be defined BEFORE the `load_monitor` actor
  </action>
  <verify>
Run `cd /Users/sn0w/Documents/dev/snow && cargo run --bin meshc -- build mesher/ 2>&1` and confirm no NEW compilation errors. Verify:
- `grep 'event_processor_worker' mesher/ingestion/pipeline.mpl` shows function definition and Node.spawn reference
- `grep 'Node.spawn' mesher/ingestion/pipeline.mpl` shows the spawn call in try_remote_spawn
  </verify>
  <done>
try_remote_spawn calls Node.spawn(target, event_processor_worker) to spawn a remote EventProcessor worker on a peer node. event_processor_worker looks up the local PipelineRegistry and starts an EventProcessor using the local pool. No PoolHandle is sent across nodes. CLUSTER-05 requirement is now fully satisfied.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Node.monitor calls for peer health tracking in load_monitor</name>
  <files>mesher/ingestion/pipeline.mpl</files>
  <action>
Add Node.monitor calls to the load_monitor actor so it receives NODEDOWN notifications when peer nodes go down.

**Part A: Add monitor_peer helper function**

Add a helper function `monitor_peer` BEFORE the `load_monitor` actor definition (after `try_remote_spawn`). This function calls Node.monitor for a given peer node name.

```
# Monitor a peer node for NODEDOWN events (CLUSTER-05 health tracking).
# Node.monitor(node_name) registers the calling process to receive
# NODEDOWN notifications when the specified node disconnects.
# Returns 0 on success, 1 on failure.
fn monitor_peer(node_name :: String) do
  let result = Node.monitor(node_name)
  if result == 0 do
    let _ = println("[Mesher] Monitoring peer: " <> node_name)
    0
  else
    let _ = println("[Mesher] Failed to monitor peer: " <> node_name)
    0
  end
end
```

**Part B: Add monitor_all_peers loop helper**

Add a helper that loops through a list of peers and monitors each one:

```
# Monitor all peers in a list by index.
fn monitor_all_peers(nodes, i :: Int, total :: Int) do
  if i < total do
    let node_name = List.get(nodes, i)
    let _ = monitor_peer(node_name)
    monitor_all_peers(nodes, i + 1, total)
  else
    0
  end
end
```

**Part C: Integrate monitoring into load_monitor**

Modify the `load_monitor` actor to:
1. Track the previous peer count to detect changes
2. Call monitor_all_peers when new peers appear (node_count increases)
3. Log when peers are lost (node_count decreases -- NODEDOWN detected)

The load_monitor actor currently takes `(pool :: PoolHandle, threshold :: Int)`. Add a third parameter `prev_peers :: Int` to track the previous peer count for change detection.

Replace the current load_monitor actor definition with:

```
actor load_monitor(pool :: PoolHandle, threshold :: Int, prev_peers :: Int) do
  Timer.sleep(5000)

  let reg_pid = Process.whereis("mesher_registry")
  let event_count = PipelineRegistry.get_event_count(reg_pid)
  let _ = PipelineRegistry.reset_event_count(reg_pid)

  let nodes = Node.list()
  let node_count = List.length(nodes)

  let _ = log_load_status(event_count, node_count)

  # Detect peer changes and set up monitoring for new peers
  if node_count > prev_peers do
    let _ = println("[Mesher] New peers detected (" <> String.from(prev_peers) <> " -> " <> String.from(node_count) <> "), setting up monitors")
    let _ = monitor_all_peers(nodes, 0, node_count)
    0
  else
    if node_count < prev_peers do
      let _ = println("[Mesher] Peer lost (" <> String.from(prev_peers) <> " -> " <> String.from(node_count) <> ") -- NODEDOWN detected")
      0
    else
      0
    end
  end

  if node_count > 0 do
    if event_count > threshold do
      try_remote_spawn(nodes)
    else
      0
    end
  else
    0
  end

  load_monitor(pool, threshold, node_count)
end
```

Key changes:
- Added `prev_peers :: Int` parameter (starts at 0 on first call)
- When node_count > prev_peers: new peers appeared, call monitor_all_peers to set up Node.monitor for each
- When node_count < prev_peers: peers were lost, log NODEDOWN detection
- Recursive call passes `node_count` as the new `prev_peers` value

**Part D: Update spawn calls for load_monitor**

Update ALL spawn calls for load_monitor to include the new third argument (initial prev_peers = 0):

1. In `start_pipeline` (around line 391): Change `spawn(load_monitor, pool, 100)` to `spawn(load_monitor, pool, 100, 0)`
2. In `restart_all_services` (around line 328): Change `spawn(load_monitor, pool, 100)` to `spawn(load_monitor, pool, 100, 0)`

Important constraints:
- `monitor_peer` and `monitor_all_peers` must be defined BEFORE `load_monitor` (define-before-use)
- They should be placed after `try_remote_spawn` and before `load_monitor`
- Node.monitor signature: `fn(String) -> Int` (node_name -> 0 success, 1 failure)
- Node.monitor registers the calling actor (load_monitor) to receive NODEDOWN_TAG messages from the runtime when the monitored node disconnects
  </action>
  <verify>
Run `cd /Users/sn0w/Documents/dev/snow && cargo run --bin meshc -- build mesher/ 2>&1` and confirm no NEW compilation errors. Verify:
- `grep 'Node.monitor' mesher/ingestion/pipeline.mpl` shows monitor call in monitor_peer
- `grep 'monitor_peer' mesher/ingestion/pipeline.mpl` shows function definition and call
- `grep 'monitor_all_peers' mesher/ingestion/pipeline.mpl` shows loop helper
- `grep 'NODEDOWN' mesher/ingestion/pipeline.mpl` shows detection log
- `grep 'prev_peers' mesher/ingestion/pipeline.mpl` shows parameter in load_monitor
- `grep 'spawn(load_monitor' mesher/ingestion/pipeline.mpl` shows updated spawn calls with 4 args
  </verify>
  <done>
load_monitor tracks peer count changes between iterations. When new peers appear, it calls Node.monitor on each peer for NODEDOWN notifications. When peers are lost, it logs NODEDOWN detection. All spawn calls updated with the new prev_peers parameter (initial value 0). Node monitoring gap is closed.
  </done>
</task>

</tasks>

<verification>
1. `meshc build mesher/` compiles with no new errors beyond pre-existing baseline
2. `grep 'Node.spawn' mesher/ingestion/pipeline.mpl` returns the spawn call in try_remote_spawn
3. `grep 'event_processor_worker' mesher/ingestion/pipeline.mpl` returns function definition
4. `grep 'Node.monitor' mesher/ingestion/pipeline.mpl` returns monitor call in monitor_peer
5. `grep 'NODEDOWN' mesher/ingestion/pipeline.mpl` returns detection log message
6. `grep 'prev_peers' mesher/ingestion/pipeline.mpl` returns load_monitor parameter
7. `grep 'spawn(load_monitor' mesher/ingestion/pipeline.mpl` shows all spawn calls have 4 arguments
</verification>

<success_criteria>
- try_remote_spawn calls Node.spawn(target, event_processor_worker) to spawn a remote processor
- event_processor_worker is a zero-arg function that looks up its own node's registry and pool
- No PoolHandle sent across nodes (event_processor_worker uses Process.whereis locally)
- load_monitor calls Node.monitor for each peer to track health
- NODEDOWN detection logged when peer count decreases
- All load_monitor spawn calls updated with prev_peers argument
- Application compiles cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/94-multi-node-clustering/94-04-SUMMARY.md`
</output>
