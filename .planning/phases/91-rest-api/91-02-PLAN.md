---
phase: 91-rest-api
plan: 02
type: execute
wave: 2
depends_on: ["91-01"]
files_modified:
  - mesher/api/dashboard.mpl
  - mesher/api/detail.mpl
  - mesher/storage/queries.mpl
  - mesher/main.mpl
autonomous: true

must_haves:
  truths:
    - "Dashboard endpoint returns event volume bucketed by hour or day for a project"
    - "Dashboard endpoint returns error breakdown by level for a project"
    - "Dashboard endpoint returns top issues ranked by frequency"
    - "Dashboard endpoint returns event breakdown by tag (environment, release)"
    - "Dashboard endpoint returns per-issue event timeline"
    - "Dashboard endpoint returns project health summary (unresolved count, 24h events, new today)"
    - "User can view full event payload including exception, stacktrace, breadcrumbs, tags, extra, and user_context"
    - "User can view formatted stack trace frames with file, line, and function info"
    - "User can navigate between events within an issue via next/previous IDs"
  artifacts:
    - path: "mesher/api/dashboard.mpl"
      provides: "Dashboard aggregation HTTP handlers (DASH-01..06)"
    - path: "mesher/api/detail.mpl"
      provides: "Event detail and navigation HTTP handlers (DETAIL-01..06)"
    - path: "mesher/storage/queries.mpl"
      provides: "Extended with dashboard aggregation and event detail query functions"
    - path: "mesher/main.mpl"
      provides: "Updated with dashboard and detail route registration"
  key_links:
    - from: "mesher/api/dashboard.mpl"
      to: "mesher/storage/queries.mpl"
      via: "imports dashboard aggregation query functions"
    - from: "mesher/api/detail.mpl"
      to: "mesher/storage/queries.mpl"
      via: "imports event detail and navigation query functions"
    - from: "mesher/main.mpl"
      to: "mesher/api/dashboard.mpl"
      via: "imports and registers dashboard route handlers"
    - from: "mesher/main.mpl"
      to: "mesher/api/detail.mpl"
      via: "imports and registers detail route handlers"
---

<objective>
Implement dashboard aggregation endpoints (DASH-01 through DASH-06) and event detail/navigation endpoints (DETAIL-01 through DETAIL-06).

Purpose: Provide dashboard metrics (event volume over time, error breakdown, top issues, tag analysis, health summary) and enable full event inspection with stack traces, breadcrumbs, tags, user context, and next/previous navigation within an issue.

Output: New `api/dashboard.mpl` with 6 dashboard endpoints, new `api/detail.mpl` with event detail + navigation endpoints, extended `queries.mpl` with aggregation and detail queries, updated `main.mpl`.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/91-rest-api/91-RESEARCH.md
@.planning/phases/91-rest-api/91-01-SUMMARY.md

@mesher/ingestion/routes.mpl
@mesher/storage/queries.mpl
@mesher/ingestion/pipeline.mpl
@mesher/main.mpl
@mesher/types/event.mpl
@mesher/api/search.mpl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add dashboard and event detail query functions to queries.mpl</name>
  <files>mesher/storage/queries.mpl</files>
  <action>
Add the following query functions to the END of storage/queries.mpl (after the functions added in Plan 01). All use parameterized queries. All dashboard queries include a default 24-hour time window for events.

**Dashboard queries (DASH-01..06):**

1. `pub fn event_volume_hourly(pool :: PoolHandle, project_id :: String, bucket :: String)` -- DASH-01
   - `bucket` param is either "hour" or "day" (passed from handler based on ?bucket=hour query param)
   - SQL: `SELECT date_trunc($2, received_at)::text AS bucket, count(*)::text AS count FROM events WHERE project_id = $1::uuid AND received_at > now() - interval '24 hours' GROUP BY bucket ORDER BY bucket`
   - NOTE: For daily buckets, handler passes bucket="day" and a wider interval may be desired. Keep 24h default for simplicity; the handler can override via a start time param later.
   - Returns List<Map<String, String>>!String

2. `pub fn error_breakdown_by_level(pool :: PoolHandle, project_id :: String)` -- DASH-02
   - SQL: `SELECT level, count(*)::text AS count FROM events WHERE project_id = $1::uuid AND received_at > now() - interval '24 hours' GROUP BY level ORDER BY count DESC`
   - Returns List<Map<String, String>>!String

3. `pub fn top_issues_by_frequency(pool :: PoolHandle, project_id :: String, limit_str :: String)` -- DASH-03
   - SQL: `SELECT id::text, title, level, status, event_count::text, last_seen::text FROM issues WHERE project_id = $1::uuid AND status = 'unresolved' ORDER BY event_count DESC LIMIT $2::int`
   - Returns List<Map<String, String>>!String

4. `pub fn event_breakdown_by_tag(pool :: PoolHandle, project_id :: String, tag_key :: String)` -- DASH-04
   - SQL: `SELECT tags->>$2 AS tag_value, count(*)::text AS count FROM events WHERE project_id = $1::uuid AND received_at > now() - interval '24 hours' AND tags ? $2 GROUP BY tag_value ORDER BY count DESC LIMIT 20`
   - Note: `?` is the JSONB key-exists operator in PostgreSQL. Since this is inside a SQL string passed to Pool.query, it should not conflict with Mesh's `?` operator. Use `tags ? $2` not `tags->>'key'`.
   - Returns List<Map<String, String>>!String

5. `pub fn issue_event_timeline(pool :: PoolHandle, issue_id :: String, limit_str :: String)` -- DASH-05
   - SQL: `SELECT id::text, level, message, received_at::text FROM events WHERE issue_id = $1::uuid ORDER BY received_at DESC LIMIT $2::int`
   - Returns List<Map<String, String>>!String

6. `pub fn project_health_summary(pool :: PoolHandle, project_id :: String)` -- DASH-06
   - Single row with subquery aggregates:
   - SQL: `SELECT (SELECT count(*) FROM issues WHERE project_id = $1::uuid AND status = 'unresolved')::text AS unresolved_count, (SELECT count(*) FROM events WHERE project_id = $1::uuid AND received_at > now() - interval '24 hours')::text AS events_24h, (SELECT count(*) FROM issues WHERE project_id = $1::uuid AND first_seen > now() - interval '24 hours')::text AS new_today`
   - Returns List<Map<String, String>>!String (single row)

**Event detail queries (DETAIL-01..06):**

7. `pub fn get_event_detail(pool :: PoolHandle, event_id :: String)` -- DETAIL-01..04, DETAIL-06
   - Returns the complete event with all JSONB fields.
   - SQL: `SELECT id::text, project_id::text, issue_id::text, level, message, fingerprint, COALESCE(exception::text, 'null') AS exception, COALESCE(stacktrace::text, '[]') AS stacktrace, COALESCE(breadcrumbs::text, '[]') AS breadcrumbs, COALESCE(tags::text, '{}') AS tags, COALESCE(extra::text, '{}') AS extra, COALESCE(user_context::text, 'null') AS user_context, COALESCE(sdk_name, '') AS sdk_name, COALESCE(sdk_version, '') AS sdk_version, received_at::text FROM events WHERE id = $1::uuid`
   - NOTE: This queries across all partitions by id alone. For a single detail view this is acceptable (research Open Question 1).
   - Returns List<Map<String, String>>!String

8. `pub fn get_event_neighbors(pool :: PoolHandle, issue_id :: String, received_at :: String, event_id :: String)` -- DETAIL-05
   - Gets next and previous event IDs within an issue for navigation.
   - SQL: `SELECT (SELECT id::text FROM events WHERE issue_id = $1::uuid AND (received_at, id) > ($2::timestamptz, $3::uuid) ORDER BY received_at, id LIMIT 1) AS next_id, (SELECT id::text FROM events WHERE issue_id = $1::uuid AND (received_at, id) < ($2::timestamptz, $3::uuid) ORDER BY received_at DESC, id DESC LIMIT 1) AS prev_id`
   - Returns List<Map<String, String>>!String (single row with next_id, prev_id keys)
  </action>
  <verify>Run `cd /Users/sn0w/Documents/dev/snow && cargo build 2>&1 | tail -20` to verify no compilation errors.</verify>
  <done>Eight new query functions added to queries.mpl: 6 dashboard aggregation queries (event volume, level breakdown, top issues, tag breakdown, issue timeline, health summary) and 2 event detail queries (full payload, next/prev navigation).</done>
</task>

<task type="auto">
  <name>Task 2: Create dashboard route handlers (api/dashboard.mpl)</name>
  <files>mesher/api/dashboard.mpl, mesher/main.mpl</files>
  <action>
**Create `mesher/api/dashboard.mpl`** with dashboard aggregation endpoints. Follow established handler pattern: registry -> pool -> params -> query -> serialize -> respond.

**CRITICAL:** Order functions bottom-up (leaf helpers first, pub handlers last) per decision [90-03].

**Imports:**
```
from Ingestion.Pipeline import PipelineRegistry
from Storage.Queries import event_volume_hourly, error_breakdown_by_level, top_issues_by_frequency, event_breakdown_by_tag, issue_event_timeline, project_health_summary
```

**Helper functions (define at TOP):**

1. `query_or_default(request, param :: String, default :: String) -> String` -- Same pattern as search.mpl.

2. `bucket_to_json(row) -> String` -- Serialize {bucket, count} row: `{"bucket":"...","count":N}`. Note: count is a number, no quotes around it.

3. `level_to_json(row) -> String` -- Serialize {level, count} row: `{"level":"...","count":N}`.

4. `top_issue_to_json(row) -> String` -- Serialize {id, title, level, status, event_count, last_seen} row.

5. `tag_entry_to_json(row) -> String` -- Serialize {tag_value, count} row: `{"value":"...","count":N}`. Handle COALESCE for null tag_value.

6. `timeline_event_to_json(row) -> String` -- Serialize {id, level, message, received_at} row.

7. `json_array_loop(items, i :: Int, total :: Int, acc :: String) -> String` + `to_json_array(items) -> String` -- Recursive JSON array builder (same pattern as search.mpl).

**Handler functions (define AFTER helpers):**

1. `pub fn handle_event_volume(request)` -- GET /api/v1/projects/:project_id/dashboard/volume
   - Extract: project_id, bucket (query_or_default "hour")
   - Call: event_volume_hourly(pool, project_id, bucket)
   - Map rows through bucket_to_json, to_json_array, respond 200

2. `pub fn handle_error_breakdown(request)` -- GET /api/v1/projects/:project_id/dashboard/levels
   - Extract: project_id
   - Call: error_breakdown_by_level(pool, project_id)
   - Map rows through level_to_json, to_json_array, respond 200

3. `pub fn handle_top_issues(request)` -- GET /api/v1/projects/:project_id/dashboard/top-issues
   - Extract: project_id, limit (default "10")
   - Call: top_issues_by_frequency(pool, project_id, limit)
   - Map rows through top_issue_to_json, to_json_array, respond 200

4. `pub fn handle_tag_breakdown(request)` -- GET /api/v1/projects/:project_id/dashboard/tags
   - Extract: project_id, key (required tag key like "environment" or "release")
   - If key is empty, return 400 with error message
   - Call: event_breakdown_by_tag(pool, project_id, key)
   - Map rows through tag_entry_to_json, to_json_array, respond 200

5. `pub fn handle_issue_timeline(request)` -- GET /api/v1/issues/:issue_id/timeline
   - Extract: issue_id, limit (default "50")
   - Call: issue_event_timeline(pool, issue_id, limit)
   - Map rows through timeline_event_to_json, to_json_array, respond 200

6. `pub fn handle_project_health(request)` -- GET /api/v1/projects/:project_id/dashboard/health
   - Extract: project_id
   - Call: project_health_summary(pool, project_id)
   - Single row result: extract unresolved_count, events_24h, new_today
   - Serialize: `{"unresolved_count":N,"events_24h":N,"new_today":N}` (numbers, no quotes)
   - Handle empty result as 404

**Update `mesher/main.mpl`:**
- Add import: `from Api.Dashboard import handle_event_volume, handle_error_breakdown, handle_top_issues, handle_tag_breakdown, handle_issue_timeline, handle_project_health`
- Add 6 route registrations after the search routes:
  ```
  let r = HTTP.on_get(r, "/api/v1/projects/:project_id/dashboard/volume", handle_event_volume)
  let r = HTTP.on_get(r, "/api/v1/projects/:project_id/dashboard/levels", handle_error_breakdown)
  let r = HTTP.on_get(r, "/api/v1/projects/:project_id/dashboard/top-issues", handle_top_issues)
  let r = HTTP.on_get(r, "/api/v1/projects/:project_id/dashboard/tags", handle_tag_breakdown)
  let r = HTTP.on_get(r, "/api/v1/issues/:issue_id/timeline", handle_issue_timeline)
  let r = HTTP.on_get(r, "/api/v1/projects/:project_id/dashboard/health", handle_project_health)
  ```
  </action>
  <verify>Run `cd /Users/sn0w/Documents/dev/snow && cargo build 2>&1 | tail -30` to verify compilation succeeds with all 6 dashboard routes.</verify>
  <done>Six dashboard endpoints exist and compile: event volume, error breakdown, top issues, tag breakdown, issue timeline, and project health summary. All registered in main.mpl router.</done>
</task>

<task type="auto">
  <name>Task 3: Create event detail route handlers (api/detail.mpl)</name>
  <files>mesher/api/detail.mpl, mesher/main.mpl</files>
  <action>
**Create `mesher/api/detail.mpl`** with event detail and navigation endpoints. These handlers serve the full event payload including JSONB fields.

**CRITICAL:** JSONB fields (exception, stacktrace, breadcrumbs, tags, extra, user_context) arrive from PostgreSQL as valid JSON strings. Embed them DIRECTLY in the response JSON without additional quoting (research Pitfall 5). Only String fields (id, level, message, received_at, sdk_name, sdk_version, fingerprint) get `\"` quoting.

**Imports:**
```
from Ingestion.Pipeline import PipelineRegistry
from Storage.Queries import get_event_detail, get_event_neighbors
```

**Helper functions (define at TOP):**

1. `event_detail_to_json(row) -> String` -- DETAIL-01..04, DETAIL-06
   - Builds complete event JSON including all fields.
   - String fields with `\"` quoting: id, project_id, issue_id, level, message, fingerprint, sdk_name, sdk_version, received_at
   - JSONB fields embedded RAW (no quoting): exception, stacktrace, breadcrumbs, tags, extra, user_context
   - Example output: `{"id":"abc","level":"error","exception":{"type_name":"TypeError","value":"..."},"stacktrace":[{...}],...}`
   - The stacktrace field is already a JSON array of frame objects with filename, function_name, lineno, colno, context_line, in_app -- this satisfies DETAIL-02 (formatted stack traces).
   - The breadcrumbs field is already a JSON array of breadcrumb objects -- satisfies DETAIL-03.
   - The tags field is already a JSON object of key-value pairs -- satisfies DETAIL-04.
   - The user_context field is already a JSON object with id, email, IP -- satisfies DETAIL-06.

2. `neighbors_to_json(row) -> String` -- DETAIL-05
   - Extract next_id and prev_id from row (may be empty strings for missing neighbors).
   - Output: `{"next_id":"...","prev_id":"..."}` or `{"next_id":null,"prev_id":"..."}` for missing neighbors.
   - Check if next_id/prev_id are empty strings. If empty, embed `null` (no quotes). If present, embed with `\"` quoting.

3. `build_detail_response(detail_json :: String, nav_json :: String) -> String` -- Combine event detail with navigation:
   - Output: `{"event":..., "navigation":...}` where event is the full event object and navigation has next_id/prev_id.

**Handler functions:**

1. `pub fn handle_event_detail(request)` -- GET /api/v1/events/:event_id
   - Extract: event_id (Request.param)
   - Call: get_event_detail(pool, event_id)
   - If no results, return HTTP.response(404, `{"error":"event not found"}`)
   - Extract the single row, serialize with event_detail_to_json
   - Also call get_event_neighbors(pool, issue_id, received_at, event_id) using the issue_id and received_at from the detail row
   - Combine with build_detail_response
   - Response: HTTP.response(200, combined_json)

**IMPORTANT for handle_event_detail:** The handler needs to make TWO queries (detail + neighbors). Structure this as:
- Query 1: get_event_detail -> extract row -> get issue_id and received_at from row
- Query 2: get_event_neighbors using issue_id, received_at, event_id
- Combine results into final response
- Extract the chained logic into helper functions to satisfy single-expression case arm constraint.

Define helpers: `build_event_response_from_rows(pool, event_id :: String, rows)` and `add_navigation(pool, event_id :: String, issue_id :: String, received_at :: String, detail_json :: String)` to keep case arms simple.

**Update `mesher/main.mpl`:**
- Add import: `from Api.Detail import handle_event_detail`
- Add route registration:
  ```
  let r = HTTP.on_get(r, "/api/v1/events/:event_id", handle_event_detail)
  ```
  </action>
  <verify>Run `cd /Users/sn0w/Documents/dev/snow && cargo build 2>&1 | tail -30` to verify compilation succeeds. Check that the event detail handler compiles with the two-query pattern.</verify>
  <done>Event detail endpoint returns full event payload with all JSONB fields properly embedded (exception, stacktrace, breadcrumbs, tags, user_context) plus next/previous navigation IDs. Registered in main.mpl router.</done>
</task>

</tasks>

<verification>
1. `cargo build` compiles without errors
2. All 8 query functions (6 dashboard + 2 detail) exist in queries.mpl
3. api/dashboard.mpl exports 6 pub handler functions
4. api/detail.mpl exports 1 pub handler function
5. main.mpl registers all 7 new GET routes (6 dashboard + 1 detail)
6. JSONB fields in event detail are embedded raw (not double-quoted)
7. Dashboard aggregation uses PostgreSQL date_trunc, GROUP BY, COUNT
8. Health summary returns numeric values (not string-quoted numbers)
9. Event navigation returns next_id/prev_id or null for missing neighbors
</verification>

<success_criteria>
- DASH-01: Event volume over time (hourly/daily buckets) endpoint works
- DASH-02: Error breakdown by level endpoint works
- DASH-03: Top issues by frequency endpoint works
- DASH-04: Event breakdown by tag (environment, release) endpoint works
- DASH-05: Per-issue event timeline endpoint works
- DASH-06: Project health summary (unresolved count, 24h events, new today) endpoint works
- DETAIL-01: Full event payload viewable via endpoint
- DETAIL-02: Stack traces returned with file, line, function info
- DETAIL-03: Breadcrumbs returned as chronological trail
- DETAIL-04: Tags returned as key-value pairs
- DETAIL-05: Next/previous event navigation within an issue
- DETAIL-06: User context (id, email, IP) returned on events
</success_criteria>

<output>
After completion, create `.planning/phases/91-rest-api/91-02-SUMMARY.md`
</output>
