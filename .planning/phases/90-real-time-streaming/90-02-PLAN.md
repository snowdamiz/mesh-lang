---
phase: 90-real-time-streaming
plan: 02
type: execute
wave: 2
depends_on: ["90-01"]
files_modified:
  - mesher/ingestion/ws_handler.mpl
  - mesher/ingestion/routes.mpl
  - mesher/ingestion/pipeline.mpl
  - mesher/main.mpl
autonomous: true

must_haves:
  truths:
    - "Dashboard client connecting to WS with /stream/projects/:id path is joined to the project room and receives real-time event notifications"
    - "Dashboard client can send a subscribe message with level/environment filters and only receive matching events"
    - "After successful event processing, the event is broadcast to the project room for all streaming subscribers"
    - "After issue state transitions (resolve, archive, unresolve), an issue notification is broadcast to the project room"
    - "Issue count updates are broadcast after event processing creates or updates an issue"
    - "SDK clients connecting to WS with /ingest path continue to work for event ingestion (no regression)"
  artifacts:
    - path: "mesher/ingestion/ws_handler.mpl"
      provides: "Dual-purpose WS handler: /ingest for SDK ingestion, /stream/projects/:id for dashboard streaming"
      contains: "Ws.join"
    - path: "mesher/ingestion/routes.mpl"
      provides: "Event and issue broadcasting after processing and state transitions"
      contains: "Ws.broadcast"
    - path: "mesher/ingestion/pipeline.mpl"
      provides: "StreamManager integration into pipeline startup"
      contains: "StreamManager"
    - path: "mesher/main.mpl"
      provides: "StreamManager import and stream_manager service wiring"
      contains: "StreamManager"
  key_links:
    - from: "mesher/ingestion/ws_handler.mpl"
      to: "mesher/services/stream_manager.mpl"
      via: "StreamManager.register_client on /stream path connect, StreamManager.remove_client on close"
      pattern: "StreamManager\\.register_client"
    - from: "mesher/ingestion/routes.mpl"
      to: "mesher/ingestion/ws_handler.mpl"
      via: "Ws.broadcast sends to rooms that ws_handler joins clients into"
      pattern: "Ws\\.broadcast"
    - from: "mesher/ingestion/pipeline.mpl"
      to: "mesher/services/stream_manager.mpl"
      via: "StreamManager.start in pipeline startup"
      pattern: "StreamManager\\.start"
---

<objective>
Wire the subscription protocol into the WebSocket handler, add event and issue broadcasting to routes, and integrate the StreamManager service into the pipeline.

Purpose: This plan connects all the pieces built in Plan 01 (typechecker room functions + StreamManager service) into the running Mesher application. Dashboard clients subscribe via WebSocket and receive real-time event notifications, issue state change notifications, and issue count updates. SDK ingestion clients continue to work unchanged via path-based routing.

Output: Complete real-time streaming system with subscription protocol, event broadcasting, issue notifications, and pipeline integration.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/90-real-time-streaming/90-RESEARCH.md
@.planning/phases/90-real-time-streaming/90-01-SUMMARY.md

# Source files to modify
@mesher/ingestion/ws_handler.mpl
@mesher/ingestion/routes.mpl
@mesher/ingestion/pipeline.mpl
@mesher/main.mpl
@mesher/services/stream_manager.mpl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement subscription protocol in WS handler and wire pipeline</name>
  <files>mesher/ingestion/ws_handler.mpl, mesher/ingestion/pipeline.mpl, mesher/main.mpl</files>
  <action>
**Part A: Modify ws_handler.mpl for dual-purpose path routing**

The WS server is shared on port 8081. Use path-based routing to distinguish SDK ingestion clients from dashboard streaming clients:
- `/ingest` path -> existing event ingestion behavior (current ws_on_message logic)
- `/stream/projects/:id` path -> streaming subscription (new behavior)

Modify `ws_on_connect`:
- Parse the `path` parameter to determine client type
- If path starts with `/stream/projects/`, extract project_id (split on "/" and take index 3)
- Call `Ws.join(conn, "project:" <> project_id)` to join the project room
- Register the connection in StreamManager via `StreamManager.register_client(stream_mgr_pid, conn, project_id, "", "")` (empty filters = accept all initially)
- Return conn to accept the connection
- If path is `/ingest` or any other path, use existing auth check behavior

Modify `ws_on_message`:
- Look up whether this connection is a streaming client via `StreamManager.is_stream_client(stream_mgr_pid, conn)`
- If streaming client: parse the message as a subscription update. Handle JSON messages:
  - `{"type":"subscribe","filters":{"level":"error","environment":"production"}}` -> update filters in StreamManager
  - For the filter update, extract level and environment from the JSON body. Since Mesh lacks nested JSON parsing, use PostgreSQL jsonb extraction pattern (same as handle_assign_issue in routes.mpl): `Pool.query(pool, "SELECT COALESCE($1::jsonb->'filters'->>'level', '') AS level, COALESCE($1::jsonb->'filters'->>'environment', '') AS env", [message])`. Then call `StreamManager.register_client` again with the new filters (re-registration overwrites).
  - Send confirmation: `ws_write(conn, "{\"type\":\"filters_updated\"}")`
- If NOT a streaming client: existing event ingestion behavior (route to EventProcessor)

Modify `ws_on_close`:
- Call `StreamManager.remove_client(stream_mgr_pid, conn)` to clean up connection state
- Note: `Ws.join` auto-cleanup on disconnect is handled by rooms.rs (cleanup_connection), but StreamManager state also needs cleanup

Add imports at the top:
```mesh
from Services.StreamManager import StreamManager
```

Note: The StreamManager PID lookup uses `Process.whereis("stream_manager")` -- registered in pipeline.mpl.

**Part B: Modify pipeline.mpl to include StreamManager**

Add import:
```mesh
from Services.StreamManager import StreamManager
```

Start StreamManager in `start_pipeline()`:
- Before the PipelineRegistry creation, start StreamManager: `let stream_mgr_pid = StreamManager.start()`
- Register by name: `let _ = Process.register("stream_manager", stream_mgr_pid)`
- Log: `println("[Mesher] StreamManager started")`

Also add StreamManager restart in `restart_all_services()`:
- Same pattern: `let stream_mgr_pid = StreamManager.start()` + register

**Part C: Update main.mpl imports**

Add `StreamManager` to imports if needed by any direct usage. The main.mpl primarily delegates to pipeline.mpl, so it may only need the import if StreamManager is referenced directly. Check if the existing `from Ingestion.WsHandler import ...` already covers the transitive dependency. If ws_handler.mpl imports StreamManager itself, main.mpl may not need a direct import.

Important Mesh constraints to follow:
- Single-expression case arms (extract helpers for multi-line logic)
- Use explicit case matching instead of ? operator for Result handling
- Use String.split for path parsing (no regex)
- Helper functions for anything beyond simple expressions
  </action>
  <verify>Build the Mesher application: `cd mesher && ../target/debug/meshc build .` (or run `cargo run -- build mesher/`). Verify no compilation errors. Check that ws_handler.mpl properly imports StreamManager and uses Ws.join. Check that pipeline.mpl starts and registers StreamManager.</verify>
  <done>WS handler routes /stream/projects/:id connections to room subscriptions and /ingest connections to event ingestion. StreamManager is started and registered in pipeline.mpl. Filter updates are handled via subscription messages. SDK ingestion continues to work unchanged.</done>
</task>

<task type="auto">
  <name>Task 2: Add event broadcasting after event processing</name>
  <files>mesher/ingestion/routes.mpl</files>
  <action>
Modify `mesher/ingestion/routes.mpl` to broadcast event notifications and issue count updates to streaming clients after successful event processing.

**Event broadcasting (STREAM-01, STREAM-04):**

Modify `route_to_processor` to broadcast after successful processing:
```mesh
fn route_to_processor(processor_pid, project_id :: String, writer_pid, body :: String) do
  let result = EventProcessor.process_event(processor_pid, project_id, writer_pid, body)
  case result do
    Ok(issue_id) ->
      broadcast_event(project_id, issue_id, body)
    Err(reason) -> bad_request_response(reason)
  end
end
```

Add helper `broadcast_event`:
```mesh
fn broadcast_event(project_id :: String, issue_id :: String, body :: String) do
  let room = "project:" <> project_id
  let notification = "{\"type\":\"event\",\"issue_id\":\"" <> issue_id <> "\",\"data\":" <> body <> "}"
  let _ = Ws.broadcast(room, notification)
  broadcast_issue_count(project_id)
  accepted_response()
end
```

Add helper `broadcast_issue_count`:
```mesh
fn broadcast_issue_count(project_id :: String) do
  let reg_pid = Process.whereis("mesher_registry")
  let pool = PipelineRegistry.get_pool(reg_pid)
  let count_result = Pool.query(pool, "SELECT count(*)::text AS cnt FROM issues WHERE project_id = $1::uuid AND status = 'unresolved'", [project_id])
  case count_result do
    Ok(rows) -> broadcast_count_from_rows(project_id, rows)
    Err(_) -> 0
  end
end
```

Add helper `broadcast_count_from_rows`:
```mesh
fn broadcast_count_from_rows(project_id :: String, rows) do
  if List.length(rows) > 0 do
    let count = Map.get(List.head(rows), "cnt")
    let room = "project:" <> project_id
    let _ = Ws.broadcast(room, "{\"type\":\"issue_count\",\"project_id\":\"" <> project_id <> "\",\"count\":" <> count <> "}")
    0
  else
    0
  end
end
```

Important: Remember the single-expression case arm constraint. Each case arm must be a single expression or a function call. Extract multi-line logic into helper functions.

Do NOT broadcast inside EventProcessor service -- broadcast AFTER the call returns in the route handler to avoid bottlenecking the single EventProcessor actor on network I/O.
  </action>
  <verify>Build the Mesher application: `cd mesher && ../target/debug/meshc build .` (or run `cargo run -- build mesher/`). Verify no compilation errors. Check that routes.mpl uses Ws.broadcast after EventProcessor returns Ok. Verify the broadcast helpers follow single-expression case arm constraints.</verify>
  <done>Event processing broadcasts event notifications and issue count updates to project rooms. Broadcasting happens AFTER EventProcessor.process_event returns, not inside the service actor. SDK ingestion flow is unchanged.</done>
</task>

<task type="auto">
  <name>Task 3: Add issue state change broadcasting to route handlers</name>
  <files>mesher/ingestion/routes.mpl</files>
  <action>
Modify `mesher/ingestion/routes.mpl` to broadcast issue state change notifications to streaming clients after successful issue state transitions.

**Issue state change broadcasting (STREAM-03):**

Add helper `broadcast_issue_update`:
```mesh
fn broadcast_issue_update(pool, issue_id :: String, action :: String) do
  let rows_result = Pool.query(pool, "SELECT project_id::text FROM issues WHERE id = $1::uuid", [issue_id])
  case rows_result do
    Ok(rows) -> broadcast_update_from_rows(rows, issue_id, action)
    Err(_) -> 0
  end
end
```

Add helper `broadcast_update_from_rows`:
```mesh
fn broadcast_update_from_rows(rows, issue_id :: String, action :: String) do
  if List.length(rows) > 0 do
    let project_id = Map.get(List.head(rows), "project_id")
    let room = "project:" <> project_id
    let msg = "{\"type\":\"issue\",\"action\":\"" <> action <> "\",\"issue_id\":\"" <> issue_id <> "\"}"
    let _ = Ws.broadcast(room, msg)
    0
  else
    0
  end
end
```

Modify issue state transition handlers to broadcast after success. For each of `handle_resolve_issue`, `handle_archive_issue`, `handle_unresolve_issue`, `handle_discard_issue`:

Extract the success path into a helper that broadcasts then returns the response. For example for resolve:
```mesh
fn resolve_success(pool, issue_id :: String, n :: Int) do
  let _ = broadcast_issue_update(pool, issue_id, "resolved")
  HTTP.response(200, "{\"status\":\"ok\",\"affected\":" <> String.from(n) <> "}")
end
```

And update handle_resolve_issue to call it:
```mesh
pub fn handle_resolve_issue(request) do
  let reg_pid = Process.whereis("mesher_registry")
  let pool = PipelineRegistry.get_pool(reg_pid)
  let issue_id = Request.param(request, "id")
  let result = resolve_issue(pool, issue_id)
  case result do
    Ok(n) -> resolve_success(pool, issue_id, n)
    Err(e) -> HTTP.response(500, "{\"error\":\"" <> e <> "\"}")
  end
end
```

Apply the same pattern to archive (action "archived"), unresolve (action "unresolved"), and discard (action "discarded").

Do NOT modify handle_delete_issue or handle_assign_issue -- delete removes the issue entirely (no room to broadcast to), and assign updates state but is not a status transition.

Important: Remember the single-expression case arm constraint. Each case arm must be a single expression or a function call. Extract multi-line logic into helper functions.
  </action>
  <verify>Build the Mesher application: `cd mesher && ../target/debug/meshc build .` (or run `cargo run -- build mesher/`). Verify no compilation errors. Check that routes.mpl broadcasts after resolve/archive/unresolve/discard state transitions. Verify the broadcast helpers follow single-expression case arm constraints.</verify>
  <done>Issue state transitions (resolve, archive, unresolve, discard) broadcast issue update notifications to the project room. All broadcasting happens AFTER the state transition query succeeds, not inside service actors.</done>
</task>

</tasks>

<verification>
1. Mesher application compiles successfully with all modifications
2. WS handler distinguishes /ingest from /stream/projects/:id paths
3. Event processing triggers Ws.broadcast to project rooms
4. Issue state transitions trigger Ws.broadcast with action notifications
5. Issue count updates are broadcast after event processing
6. StreamManager is started and registered in pipeline startup
7. No regressions in existing HTTP or WS ingestion endpoints
</verification>

<success_criteria>
- Dashboard WS clients on /stream/projects/:id are joined to project rooms and receive event notifications
- Filter updates via subscription messages are tracked in StreamManager
- Event broadcasts occur after EventProcessor.process_event returns Ok
- Issue broadcasts occur after resolve/archive/unresolve/discard state transitions
- Issue count updates are broadcast after event processing
- Existing SDK ingestion via /ingest path continues to work
- Application compiles and all services start correctly
</success_criteria>

<output>
After completion, create `.planning/phases/90-real-time-streaming/90-02-SUMMARY.md`
</output>
