---
phase: 90-real-time-streaming
plan: 03
type: execute
wave: 2
depends_on: ["90-01"]
files_modified:
  - mesher/services/stream_manager.mpl
autonomous: true

must_haves:
  truths:
    - "Slow streaming clients have their messages buffered and drained periodically via a ticker actor"
    - "When a connection's buffer exceeds max_buffer, oldest events are dropped to stay within capacity"
    - "Buffer drain sends each buffered message via Ws.send and clears the buffer on success"
    - "If Ws.send returns -1 (error), the connection is cleaned up via StreamManager.remove_client"
  artifacts:
    - path: "mesher/services/stream_manager.mpl"
      provides: "BufferMessage cast handler and stream_drain_ticker actor for periodic buffer flushing"
      contains: "stream_drain_ticker"
  key_links:
    - from: "mesher/services/stream_manager.mpl"
      to: "mesher/services/stream_manager.mpl"
      via: "stream_drain_ticker calls StreamManager.flush_buffers which drains via Ws.send"
      pattern: "stream_drain_ticker"
    - from: "mesher/ingestion/routes.mpl"
      to: "mesher/services/stream_manager.mpl"
      via: "broadcast helpers can buffer messages for slow clients via StreamManager.buffer_message"
      pattern: "StreamManager\\.buffer_message"
---

<objective>
Implement the backpressure buffer drain mechanism for slow streaming clients (STREAM-05).

Purpose: Plan 01 created the StreamManager service with ConnectionState containing buffer/buffer_len/max_buffer fields, but no mechanism to actually buffer messages or drain them. This plan adds the BufferMessage cast handler (to queue messages with drop-oldest when max_buffer is exceeded) and a stream_drain_ticker actor (following the flush_ticker pattern from writer.mpl) that periodically drains buffered messages via Ws.send.

Output: Complete backpressure system where slow clients have messages buffered, old events are dropped when capacity is exceeded, and a ticker actor periodically flushes buffered messages.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/90-real-time-streaming/90-RESEARCH.md
@.planning/phases/90-real-time-streaming/90-01-SUMMARY.md

# Reference pattern
@mesher/services/writer.mpl (flush_ticker actor pattern)
@mesher/services/stream_manager.mpl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add buffer management and drain ticker to StreamManager</name>
  <files>mesher/services/stream_manager.mpl</files>
  <action>
Modify `mesher/services/stream_manager.mpl` to add buffer management handlers and a drain ticker actor. Follow the exact pattern from `mesher/services/writer.mpl` (flush_ticker + StorageWriter.flush).

**Add helper function for buffering with drop-oldest (outside service block):**

```mesh
fn buffer_message_for_conn(state :: StreamState, conn :: Int, msg :: String) -> StreamState do
  let cs = Map.get(state.connections, conn)
  let appended = List.append(cs.buffer, msg)
  let new_len = cs.buffer_len + 1
  # Drop oldest if over capacity (same pattern as StorageWriter)
  let buf = if new_len > cs.max_buffer do List.drop(appended, new_len - cs.max_buffer) else appended end
  let blen = if new_len > cs.max_buffer do cs.max_buffer else new_len end
  let new_cs = ConnectionState { project_id: cs.project_id, level_filter: cs.level_filter, env_filter: cs.env_filter, buffer: buf, buffer_len: blen, max_buffer: cs.max_buffer }
  let new_conns = Map.put(state.connections, conn, new_cs)
  StreamState { connections: new_conns }
end
```

**Add helper function for draining all connection buffers (outside service block):**

```mesh
fn drain_all_buffers(state :: StreamState) -> StreamState do
  let conns = Map.keys(state.connections)
  drain_connections_loop(state, conns, 0, List.length(conns))
end

fn drain_connections_loop(state :: StreamState, conns, i :: Int, total :: Int) -> StreamState do
  if i < total do
    let conn = List.get(conns, i)
    let new_state = drain_single_connection(state, conn)
    drain_connections_loop(new_state, conns, i + 1, total)
  else
    state
  end
end

fn drain_single_connection(state :: StreamState, conn :: Int) -> StreamState do
  let cs = Map.get(state.connections, conn)
  if cs.buffer_len > 0 do
    let send_ok = send_buffer_loop(conn, cs.buffer, 0, cs.buffer_len)
    if send_ok == 0 do
      # All sends succeeded -- clear buffer
      let cleared_cs = ConnectionState { project_id: cs.project_id, level_filter: cs.level_filter, env_filter: cs.env_filter, buffer: List.new(), buffer_len: 0, max_buffer: cs.max_buffer }
      let new_conns = Map.put(state.connections, conn, cleared_cs)
      StreamState { connections: new_conns }
    else
      # Ws.send returned -1 (connection error) -- remove this connection
      remove_client(state, conn)
    end
  else
    state
  end
end

fn send_buffer_loop(conn :: Int, buffer, i :: Int, total :: Int) -> Int do
  if i < total do
    let msg = List.get(buffer, i)
    let result = Ws.send(conn, msg)
    if result == -1 do
      -1
    else
      send_buffer_loop(conn, buffer, i + 1, total)
    end
  else
    0
  end
end
```

**Add two new handlers to the StreamManager service block:**

```mesh
  # Buffer a message for a slow client with drop-oldest backpressure (STREAM-05)
  cast BufferMessage(conn :: Int, msg :: String) do |state|
    if is_stream_client(state, conn) do
      buffer_message_for_conn(state, conn, msg)
    else
      state
    end
  end

  # Drain all connection buffers -- called by stream_drain_ticker periodically
  cast DrainBuffers() do |state|
    drain_all_buffers(state)
  end
```

**Add ticker actor (after the service block, same pattern as flush_ticker in writer.mpl):**

```mesh
# Ticker actor for periodic buffer drain (STREAM-05 backpressure).
# Uses Timer.sleep + recursive call because Timer.send_after delivers raw bytes
# that cannot match service cast dispatch tags (type_tag-based dispatch).
# Spawned alongside StreamManager in pipeline.mpl.
actor stream_drain_ticker(stream_mgr_pid, interval :: Int) do
  Timer.sleep(interval)
  StreamManager.drain_buffers(stream_mgr_pid)
  stream_drain_ticker(stream_mgr_pid, interval)
end
```

**Export the ticker actor** so pipeline.mpl can spawn it:
Add `stream_drain_ticker` to the module's public exports (or ensure it is accessible from pipeline.mpl).

Note: The drain interval should be 250ms (4 flushes/second) to balance latency vs overhead. This can be adjusted. The writer.mpl flush_ticker uses a longer interval because database writes are expensive; WS sends are cheap.
  </action>
  <verify>Build the Mesher application: `cd mesher && ../target/debug/meshc build .` (or run `cargo run -- build mesher/`). Verify no compilation errors. Check that StreamManager has BufferMessage and DrainBuffers cast handlers. Check that stream_drain_ticker actor follows the same pattern as flush_ticker in writer.mpl. Verify helper functions are outside the service block.</verify>
  <done>StreamManager has BufferMessage cast (queues messages with drop-oldest when max_buffer exceeded) and DrainBuffers cast (iterates all connections, sends buffered messages via Ws.send, clears buffers on success, removes connections on send failure). stream_drain_ticker actor periodically calls DrainBuffers following the flush_ticker pattern.</done>
</task>

<task type="auto">
  <name>Task 2: Wire stream_drain_ticker into pipeline startup</name>
  <files>mesher/ingestion/pipeline.mpl</files>
  <action>
Modify `mesher/ingestion/pipeline.mpl` to spawn the stream_drain_ticker actor after starting and registering the StreamManager service.

In `start_pipeline()`, after the StreamManager start and register lines (added by Plan 02 Task 1):
```mesh
let _ = spawn stream_drain_ticker(stream_mgr_pid, 250)
println("[Mesher] StreamManager drain ticker started (250ms interval)")
```

Also add the same spawn in `restart_all_services()` after the StreamManager restart.

Add import if not already present:
```mesh
from Services.StreamManager import stream_drain_ticker
```

This follows the exact same pattern used for flush_ticker spawning alongside StorageWriter instances. The 250ms interval provides responsive buffer draining without excessive overhead (Ws.send to an empty buffer is a no-op via the buffer_len > 0 check).
  </action>
  <verify>Build the Mesher application: `cd mesher && ../target/debug/meshc build .` (or run `cargo run -- build mesher/`). Verify pipeline.mpl spawns stream_drain_ticker with the StreamManager PID. Check that the import for stream_drain_ticker is present.</verify>
  <done>stream_drain_ticker is spawned in both start_pipeline and restart_all_services alongside StreamManager. The ticker periodically drains buffered messages for slow clients, completing the STREAM-05 backpressure mechanism.</done>
</task>

</tasks>

<verification>
1. StreamManager has BufferMessage and DrainBuffers cast handlers
2. buffer_message_for_conn implements drop-oldest when buffer exceeds max_buffer
3. drain_all_buffers iterates connections and sends buffered messages via Ws.send
4. Send failures (Ws.send returns -1) trigger connection removal
5. stream_drain_ticker actor follows flush_ticker pattern (Timer.sleep + recursive call)
6. Pipeline spawns stream_drain_ticker alongside StreamManager
7. Mesher application compiles successfully
</verification>

<success_criteria>
- Slow clients have messages buffered in StreamManager with drop-oldest backpressure
- stream_drain_ticker periodically flushes buffered messages via Ws.send
- Connections with send errors are automatically removed
- Complete STREAM-05 backpressure mechanism is operational
</success_criteria>

<output>
After completion, create `.planning/phases/90-real-time-streaming/90-03-SUMMARY.md`
</output>
