---
phase: 87-foundation
plan: 02
type: execute
wave: 2
depends_on: ["87-01"]
files_modified:
  - mesher/services/org_service.mpl
  - mesher/services/project_service.mpl
  - mesher/services/user_service.mpl
  - mesher/storage/writer.mpl
  - mesher/main.mpl
autonomous: true

must_haves:
  truths:
    - "OrgService creates organizations and retrieves them by ID"
    - "ProjectService creates projects within orgs and generates mshr_-prefixed API keys"
    - "UserService creates users with bcrypt-hashed passwords and authenticates via email/password"
    - "UserService creates and validates opaque session tokens"
    - "StorageWriter accumulates events in a bounded buffer and flushes to PostgreSQL on size threshold or timer"
    - "StorageWriter drops oldest events when buffer exceeds capacity"
    - "StorageWriter retries failed flushes 3 times with exponential backoff then drops the batch"
    - "PartitionManager pre-creates 7 days of daily event partitions on startup and re-checks daily"
    - "main.mpl connects to PostgreSQL, runs schema creation, starts PartitionManager, and starts all services"
  artifacts:
    - path: "mesher/services/org_service.mpl"
      provides: "OrgService with create_org, get_org, list_orgs"
      contains: "service OrgService"
    - path: "mesher/services/project_service.mpl"
      provides: "ProjectService with create_project, create_api_key, get_project_by_key"
      contains: "service ProjectService"
    - path: "mesher/services/user_service.mpl"
      provides: "UserService with register, login, validate_session"
      contains: "service UserService"
    - path: "mesher/storage/writer.mpl"
      provides: "StorageWriter with batch+timer flush, bounded buffer, retry logic"
      contains: "service StorageWriter"
    - path: "mesher/main.mpl"
      provides: "Application entry point wiring all services together"
      contains: "fn main"
  key_links:
    - from: "mesher/services/org_service.mpl"
      to: "mesher/storage/queries.mpl"
      via: "Delegates to Queries.insert_org, Queries.get_org"
      pattern: "Queries\\."
    - from: "mesher/services/project_service.mpl"
      to: "mesher/storage/queries.mpl"
      via: "Delegates to Queries.insert_project, Queries.create_api_key"
      pattern: "Queries\\."
    - from: "mesher/services/user_service.mpl"
      to: "mesher/storage/queries.mpl"
      via: "Delegates to Queries.create_user, Queries.authenticate_user, Queries.create_session"
      pattern: "Queries\\."
    - from: "mesher/storage/writer.mpl"
      to: "PostgreSQL"
      via: "Pool.execute for batch INSERT"
      pattern: "Pool\\.execute"
    - from: "mesher/storage/writer.mpl"
      to: "self()"
      via: "Timer.send_after for periodic flush"
      pattern: "Timer\\.send_after"
    - from: "mesher/main.mpl"
      to: "all services"
      via: "Service.start calls + Pool.open + Schema.create_schema"
      pattern: "Pool\\.open.*create_schema"
---

<objective>
Implement all Mesher services (org, project, user, storage writer) and wire them together in the main entry point.

Purpose: Turn the data types and schema from Plan 01 into living, stateful services that handle org/project tenancy, user authentication, and batched event storage. This completes the foundation layer so that Phase 88 (Ingestion Pipeline) can immediately start sending events through the system.
Output: 5 Mesh source files implementing all services and the application entry point. The complete Mesher project compiles with `meshc build mesher/`.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/87-foundation/87-CONTEXT.md
@.planning/phases/87-foundation/87-RESEARCH.md
@.planning/phases/87-foundation/87-01-SUMMARY.md

# Reference patterns from existing Mesh test files
@tests/e2e/service_counter.mpl
@tests/e2e/supervisor_basic.mpl
@tests/e2e/stdlib_pg.mpl

# Types and storage from Plan 01
@mesher/types/event.mpl
@mesher/types/issue.mpl
@mesher/types/project.mpl
@mesher/types/user.mpl
@mesher/types/alert.mpl
@mesher/storage/schema.mpl
@mesher/storage/queries.mpl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement org, project, and user services</name>
  <files>
    mesher/services/org_service.mpl
    mesher/services/project_service.mpl
    mesher/services/user_service.mpl
  </files>
  <action>
Create `mesher/services/` subdirectory. Create all 3 service files. Each service is a Mesh `service` (GenServer actor) that holds the pool handle as state and delegates to Storage.Queries for database operations.

**mesher/services/org_service.mpl** -- Module `Services.OrgService`:

```
service OrgService do
  fn init(pool :: Int) -> Int do
    pool
  end

  call CreateOrg(name :: String, slug :: String) :: String!String do |pool|
    let result = Queries.insert_org(pool, name, slug)
    (result, pool)
  end

  call GetOrg(id :: String) :: Organization!String do |pool|
    let result = Queries.get_org(pool, id)
    (result, pool)
  end

  call ListOrgs() :: List<Organization>!String do |pool|
    let result = Queries.list_orgs(pool)
    (result, pool)
  end
end
```

Import: `from Storage.Queries import { insert_org, get_org, list_orgs }` (or import the module and call qualified). Import `from Types.Project import { Organization }`.

The service pattern: `call` handlers return a tuple `(reply_value, new_state)` where state is just the pool handle (Int). All actual logic lives in Queries -- the service is a thin actor wrapper providing message-based concurrency.

**mesher/services/project_service.mpl** -- Module `Services.ProjectService`:

Same pattern as OrgService. Calls:
- `call CreateProject(org_id :: String, name :: String, platform :: String) :: String!String` -- delegates to Queries.insert_project
- `call GetProject(id :: String) :: Project!String` -- delegates to Queries.get_project
- `call ListProjectsByOrg(org_id :: String) :: List<Project>!String` -- delegates to Queries.list_projects_by_org
- `call CreateApiKey(project_id :: String, label :: String) :: String!String` -- delegates to Queries.create_api_key, returns the generated mshr_xxx key value
- `call GetProjectByApiKey(key_value :: String) :: Project!String` -- delegates to Queries.get_project_by_api_key (used by ingestion pipeline in Phase 88 for DSN auth)
- `call RevokeApiKey(key_id :: String) :: Int!String` -- delegates to Queries.revoke_api_key

Import: Types.Project for Project, ApiKey structs. Storage.Queries for all query functions.

**mesher/services/user_service.mpl** -- Module `Services.UserService`:

Same pattern. Calls:
- `call Register(email :: String, password :: String, display_name :: String) :: String!String` -- delegates to Queries.create_user (bcrypt hashing happens in SQL via pgcrypto)
- `call Login(email :: String, password :: String) :: String!String` -- delegates to Queries.authenticate_user to verify credentials, then Queries.create_session to create a session token. Returns the session token string on success.
- `call ValidateSession(token :: String) :: Session!String` -- delegates to Queries.validate_session
- `call Logout(token :: String) :: Int!String` -- delegates to Queries.delete_session
- `call GetUser(id :: String) :: User!String` -- delegates to Queries.get_user
- `call AddMember(user_id :: String, org_id :: String, role :: String) :: String!String` -- delegates to Queries.add_member
- `call GetMembers(org_id :: String) :: List<OrgMembership>!String` -- delegates to Queries.get_members

The Login call is a two-step operation: first authenticate (returns User on success or error), then create session. If authenticate succeeds, extract user.id and create a session token. Return the token.

Import: Types.User for User, Session, OrgMembership. Storage.Queries for all query functions.

Important pattern notes for ALL services:
- `call` handlers use the pattern: `call Name(args) :: ReturnType do |state| ... (reply, new_state) end`
- The state is always `Int` (the pool handle) and never changes
- Each call returns `(result, pool)` where result is the query return value
- `cast` handlers (async, no reply) use: `cast Name(args) do |state| ... new_state end`
- Start a service with `ServiceName.start(pool)` which returns the actor PID
- Call a service with `ServiceName.method_name(pid, args)` -- generated snake_case from PascalCase call name
  </action>
  <verify>
Read all 3 service files. Verify each has: module declaration, proper imports, service block with init and call handlers, correct return tuple pattern (reply, state). Count total call handlers: OrgService (3) + ProjectService (6) + UserService (7) = 16 call handlers.
  </verify>
  <done>Three service files implement thin actor wrappers around Storage.Queries: OrgService (org CRUD), ProjectService (project CRUD + API key management), UserService (register/login/session/membership). Each service holds the pool handle as state and delegates all database operations to the query layer.</done>
</task>

<task type="auto">
  <name>Task 2: Implement StorageWriter service and main entry point</name>
  <files>
    mesher/storage/writer.mpl
    mesher/main.mpl
  </files>
  <action>
**mesher/storage/writer.mpl** -- Module `Storage.Writer`:

This is the most complex service in the foundation. It implements the per-project StorageWriter actor (LOCKED: per-project writers for isolation) with bounded buffer (LOCKED: drop-oldest backpressure) and dual flush triggers (LOCKED: size + timer).

Define the state struct:
```
struct WriterState do
  pool :: Int
  project_id :: String
  buffer :: List<String>    # List of JSON-encoded event strings for batch INSERT
  buffer_len :: Int         # Track length explicitly (avoid repeated List.length calls)
  batch_size :: Int         # 50 (discretion)
  max_buffer :: Int         # 500 (discretion)
  flush_interval :: Int     # 5000ms (discretion)
end
```

Note: buffer stores JSON-encoded event strings (not Event structs) because the INSERT uses JSONB columns. Each string in the buffer is already formatted for the SQL INSERT values.

Actually, simpler approach: store raw field values needed for INSERT. The StorageWriter receives events as cast messages with all the fields needed for the INSERT statement. Store each event as a pre-formatted SQL values tuple string, or better: accumulate a list of event field tuples and build a multi-row INSERT on flush.

Revised approach -- use a simpler buffer of event data maps:

```
struct WriterState do
  pool :: Int
  project_id :: String
  buffer :: List<Map<String, String>>   # Each map has all event column values
  buffer_len :: Int
  batch_size :: Int         # 50
  max_buffer :: Int         # 500
  flush_interval :: Int     # 5000ms
end
```

Service definition:
```
service StorageWriter do
  fn init(pool :: Int, project_id :: String) -> WriterState do
    Timer.send_after(self(), 5000, "flush")
    WriterState {
      pool: pool,
      project_id: project_id,
      buffer: [],
      buffer_len: 0,
      batch_size: 50,
      max_buffer: 500,
      flush_interval: 5000
    }
  end
```

Cast handlers:
- `cast Store(event_data :: Map<String, String>)` -- Add event to buffer. If buffer_len >= max_buffer, drop the oldest entry (LOCKED: drop-oldest). Use List.take to cap the buffer. If buffer_len >= batch_size after adding, call flush_batch immediately. Return new state.
- `cast Flush()` -- If buffer is non-empty, call flush_batch. Whether flush succeeds or fails, clear the buffer. Re-schedule timer: `Timer.send_after(self(), state.flush_interval, "flush")`. Return new state with empty buffer.

**IMPORTANT about Timer.send_after and service cast:** The research notes an open question about whether Timer.send_after string messages ("flush") work with service cast handlers. The service uses typed cast handlers (e.g., `cast Flush()`). If the timer sends a string message "flush", it may not match the cast handler.

**Workaround:** Define the cast as `cast Flush()` and in the init, send the timer message. The timer delivery mechanism needs to match the cast dispatch. Based on the research open question (#2), this may need to use a sum type message or test empirically. For now, implement with `Timer.send_after(self(), 5000, "flush")` and the `cast Flush()` handler. If this does not work at runtime, the executor should try defining a custom message type and adjusting.

Helper function `flush_batch`:
```
fn flush_batch(pool :: Int, project_id :: String, events :: List<Map<String, String>>) -> Int!String do
  # Build a multi-row INSERT statement
  # For each event in the list, format as VALUES clause
  # INSERT INTO events (project_id, issue_id, level, message, fingerprint, ...) VALUES ($1, ...), ($N, ...)
  # ... implementation details
end
```

For the batch INSERT, the simplest approach that works with Mesh's Pool.execute (which takes `List<String>` params with positional $N placeholders): iterate over the buffer and execute individual INSERTs in sequence. A true multi-row INSERT would require dynamic SQL construction with numbered parameters, which is complex. Start with individual inserts in a loop -- the batch benefit comes from accumulating events and flushing together rather than on every single event.

Actually, the better approach: use Pool.execute for each event in the batch. Wrap in a begin/commit transaction pattern if Mesh supports it, otherwise just loop. The key optimization is batching (reducing flush frequency), not multi-row INSERT.

```
fn flush_batch(pool :: Int, project_id :: String, events :: List<Map<String, String>>) -> Int!String do
  let results = List.map(events, fn(event) do
    Pool.execute(pool,
      "INSERT INTO events (project_id, issue_id, level, message, fingerprint, exception, stacktrace, breadcrumbs, tags, extra, user_context, sdk_name, sdk_version) VALUES ($1, $2, $3, $4, $5, $6::jsonb, $7::jsonb, $8::jsonb, $9::jsonb, $10::jsonb, $11::jsonb, $12, $13)",
      [project_id, Map.get(event, "issue_id"), Map.get(event, "level"), Map.get(event, "message"), Map.get(event, "fingerprint"), Map.get(event, "exception"), Map.get(event, "stacktrace"), Map.get(event, "breadcrumbs"), Map.get(event, "tags"), Map.get(event, "extra"), Map.get(event, "user_context"), Map.get(event, "sdk_name"), Map.get(event, "sdk_version")])
  end)
  Ok(0)
end
```

Retry logic (LOCKED: retry N times with backoff, drop on failure):
```
fn flush_with_retry(pool :: Int, project_id :: String, events :: List<Map<String, String>>, attempt :: Int) -> Int!String do
  let result = flush_batch(pool, project_id, events)
  case result do
    Ok(n) -> Ok(n)
    Err(e) ->
      if attempt >= 3 do
        # Drop batch after 3 retries (LOCKED: prevents unbounded memory growth)
        println("[StorageWriter] Dropping batch of " <> String.from_int(List.length(events)) <> " events for project " <> project_id <> " after 3 retries: " <> e)
        Ok(0)
      else
        # Exponential backoff: 100ms, 500ms, 2000ms (discretion)
        let delay = case attempt do
          1 -> 100
          2 -> 500
          _ -> 2000
        end
        Timer.sleep(delay)
        flush_with_retry(pool, project_id, events, attempt + 1)
      end
  end
end
```

Note: Using `Timer.sleep` for backoff delay. If Mesh does not have `Timer.sleep`, use the OS sleep or a blocking wait. The research mentions `Timer.send_after` but not `Timer.sleep`. If `Timer.sleep` does not exist, the executor should check for alternatives (perhaps the retry logic needs to be async via self-messaging). For simplicity, implement with a synchronous retry loop first.

**mesher/main.mpl** -- Entry point (no module declaration, it's the root):

The main function wires everything together:

```
fn main() do
  # 1. Connect to PostgreSQL
  let pool_result = Pool.open("postgres://mesh:mesh@localhost:5432/mesher", 2, 10, 5000)
  let pool = case pool_result do
    Ok(p) -> p
    Err(e) ->
      println("Failed to connect to PostgreSQL: " <> e)
      0  # Exit early -- in practice this would halt
  end

  # 2. Run schema creation (idempotent)
  let schema_result = Schema.create_schema(pool)
  case schema_result do
    Ok(_) -> println("[Mesher] Schema created/verified")
    Err(e) -> println("[Mesher] Schema error: " <> e)
  end

  # 3. Create initial partitions (7 days ahead)
  let partition_result = Schema.create_partitions_ahead(pool, 7)
  case partition_result do
    Ok(_) -> println("[Mesher] Partitions created")
    Err(e) -> println("[Mesher] Partition error: " <> e)
  end

  # 4. Start services
  let org_svc = OrgService.start(pool)
  println("[Mesher] OrgService started")

  let project_svc = ProjectService.start(pool)
  println("[Mesher] ProjectService started")

  let user_svc = UserService.start(pool)
  println("[Mesher] UserService started")

  println("[Mesher] Foundation ready")

  # Keep the main process alive (services run as actors)
  # In Phase 88, this will be replaced by HTTP.serve which blocks
  Timer.sleep(999999999)
end
```

Import all needed modules at the top:
```
from Storage.Schema import { create_schema, create_partitions_ahead }
from Services.OrgService import { OrgService }
from Services.ProjectService import { ProjectService }
from Services.UserService import { UserService }
```

Note on `Timer.sleep`: Research does not confirm this function exists. If it does not, use a receive block to block the main process: `receive do _ -> 0 end`. Or use an infinite loop. The executor should test and adapt.

After creating all files, attempt to compile the project:
```bash
cd /Users/sn0w/Documents/dev/snow && cargo run --bin meshc -- build mesher/
```

This will likely surface compile errors. Fix any issues:
- Import path mismatches (adjust module names to match file paths)
- Missing function signatures or type mismatches
- Syntax errors in the Mesh code
- Service call/cast handler format issues

Iterate until `meshc build mesher/` compiles successfully. The binary does not need to RUN successfully (requires a live PostgreSQL), but it MUST COMPILE.

IMPORTANT: If Timer.sleep does not exist, check the Mesh builtins for available Timer functions: `grep -r "Timer" crates/mesh-typeck/src/builtins.rs`. Use whatever is available to keep the main process alive.

IMPORTANT: If `Timer.send_after` string messages do not dispatch to service cast handlers, check how service cast dispatch works: `grep -r "cast" crates/mesh-rt/src/actor/`. Adjust the timer message format to match what the service runtime expects.
  </action>
  <verify>
1. Run `meshc build mesher/` -- must compile without errors (produces a binary)
2. Read all 5 files and verify: writer.mpl has StorageWriter service with Store/Flush cast handlers, retry logic, bounded buffer; main.mpl imports all modules and starts all services
3. Verify the binary exists: `ls mesher/mesher` or wherever meshc outputs it
4. Count total service definitions: OrgService + ProjectService + UserService + StorageWriter = 4 services
  </verify>
  <done>
Four services implemented and wired: OrgService (org CRUD), ProjectService (project + API key CRUD), UserService (auth + sessions), StorageWriter (batched event writes with bounded buffer, timer flush, retry with exponential backoff). main.mpl connects to PostgreSQL, creates schema and partitions, starts all services. The complete Mesher project compiles with `meshc build mesher/`.
  </done>
</task>

</tasks>

<verification>
1. `meshc build mesher/` compiles successfully
2. Complete project structure exists: mesher/{main.mpl, types/*.mpl, storage/*.mpl, services/*.mpl} -- 12 files total
3. All 4 services defined with correct call/cast handlers
4. StorageWriter implements: bounded buffer (500 cap), batch flush (50 threshold), timer flush (5s), retry (3x exponential backoff), drop-oldest backpressure
5. main.mpl wires Pool.open -> create_schema -> create_partitions_ahead -> start all services
6. All LOCKED decisions honored: per-project writers, drop-oldest, mshr_ keys, daily partitioning, UUIDv7, size+timer flush, bcrypt via pgcrypto
</verification>

<success_criteria>
- `meshc build mesher/` produces a binary without errors
- 4 service actors defined (OrgService, ProjectService, UserService, StorageWriter)
- StorageWriter has dual flush triggers (size=50, timer=5s), bounded buffer (500), retry (3x backoff), drop-oldest
- main.mpl is a complete application entry point
- All user-locked decisions implemented exactly as specified
- Project structure matches research recommendation
</success_criteria>

<output>
After completion, create `.planning/phases/87-foundation/87-02-SUMMARY.md`
</output>
