---
phase: 89-error-grouping-issue-lifecycle
plan: 02
type: execute
wave: 2
depends_on: ["89-01"]
files_modified:
  - mesher/storage/queries.mpl
  - mesher/ingestion/routes.mpl
  - mesher/ingestion/pipeline.mpl
  - mesher/main.mpl
autonomous: true

must_haves:
  truths:
    - "User can transition issues between unresolved, resolved, and archived states"
    - "User can assign issues to team members"
    - "User can delete issues (removes issue and all events)"
    - "User can discard issues (suppresses future events for that fingerprint)"
    - "System auto-escalates archived issues when a volume spike occurs"
  artifacts:
    - path: "mesher/storage/queries.mpl"
      provides: "Issue management queries: state transitions, assign, delete, discard, list, spike detection"
      contains: "resolve_issue"
    - path: "mesher/ingestion/routes.mpl"
      provides: "HTTP route handlers for issue management API"
      contains: "handle_resolve_issue"
    - path: "mesher/ingestion/pipeline.mpl"
      provides: "Spike detection ticker actor"
      contains: "spike_checker"
    - path: "mesher/main.mpl"
      provides: "Issue management routes registered and spike checker spawned"
      contains: "on_post.*resolve"
  key_links:
    - from: "mesher/ingestion/routes.mpl"
      to: "mesher/storage/queries.mpl"
      via: "import and call issue management queries"
      pattern: "resolve_issue|archive_issue|assign_issue"
    - from: "mesher/ingestion/pipeline.mpl"
      to: "mesher/storage/queries.mpl"
      via: "spike_checker calls check_volume_spikes"
      pattern: "check_volume_spikes"
    - from: "mesher/main.mpl"
      to: "mesher/ingestion/routes.mpl"
      via: "HTTP route registration for issue endpoints"
      pattern: "handle_.*issue"
---

<objective>
Implement issue lifecycle management API (state transitions, assignment, delete/discard) and volume spike auto-escalation for archived issues.

Purpose: Users need to manage issue states and the system must automatically detect when archived issues spike. This completes the full issue lifecycle.
Output: Issue management HTTP routes, spike detection actor, extended queries.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/89-error-grouping-issue-lifecycle/89-RESEARCH.md
@.planning/phases/89-error-grouping-issue-lifecycle/89-01-SUMMARY.md

Source files to read before implementing:
@mesher/storage/queries.mpl
@mesher/ingestion/routes.mpl
@mesher/ingestion/pipeline.mpl
@mesher/main.mpl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Issue management queries and HTTP route handlers</name>
  <files>mesher/storage/queries.mpl, mesher/ingestion/routes.mpl</files>
  <action>
**Extend `mesher/storage/queries.mpl`** with issue management queries. Follow existing query patterns (Pool.query/Pool.execute, `?` operator, Map.get on rows).

Add these pub functions:

1. `pub fn resolve_issue(pool :: PoolHandle, issue_id :: String) -> Int!String` -- ISSUE-01
   SQL: `UPDATE issues SET status = 'resolved' WHERE id = $1::uuid AND status != 'resolved'`
   Uses Pool.execute, returns affected row count.

2. `pub fn archive_issue(pool :: PoolHandle, issue_id :: String) -> Int!String` -- ISSUE-01
   SQL: `UPDATE issues SET status = 'archived' WHERE id = $1::uuid`

3. `pub fn unresolve_issue(pool :: PoolHandle, issue_id :: String) -> Int!String` -- ISSUE-01
   SQL: `UPDATE issues SET status = 'unresolved' WHERE id = $1::uuid`

4. `pub fn assign_issue(pool :: PoolHandle, issue_id :: String, user_id :: String) -> Int!String` -- ISSUE-04
   If `String.length(user_id) > 0`: SQL `UPDATE issues SET assigned_to = $2::uuid WHERE id = $1::uuid`
   Else: SQL `UPDATE issues SET assigned_to = NULL WHERE id = $1::uuid`

5. `pub fn discard_issue(pool :: PoolHandle, issue_id :: String) -> Int!String` -- ISSUE-05
   SQL: `UPDATE issues SET status = 'discarded' WHERE id = $1::uuid`
   This marks the issue so future events with this fingerprint are suppressed by the EventProcessor discard check from Plan 01.

6. `pub fn delete_issue(pool :: PoolHandle, issue_id :: String) -> Int!String` -- ISSUE-05
   Two queries (events first due to FK constraint):
   `Pool.execute(pool, "DELETE FROM events WHERE issue_id = $1::uuid", [issue_id])?`
   `Pool.execute(pool, "DELETE FROM issues WHERE id = $1::uuid", [issue_id])`

7. `pub fn list_issues_by_status(pool :: PoolHandle, project_id :: String, status :: String) -> List<Issue>!String` -- For API listing
   SQL: `SELECT id::text, project_id::text, fingerprint, title, level, status, event_count::text, first_seen::text, last_seen::text, COALESCE(assigned_to::text, '') as assigned_to FROM issues WHERE project_id = $1::uuid AND status = $2 ORDER BY last_seen DESC`
   NOTE: event_count must be cast to ::text because Row structs use all-String fields (decision [87-01]). The Issue struct has `event_count :: Int` -- check if this causes issues. If it does, the workaround is: keep event_count as String in the Map, pass it through. Actually the Issue struct has `event_count :: Int` but we construct it manually from Map.get, and Map.get returns String. We need `String.to_int(Map.get(row, "event_count"))` for the Int field. But Issue uses `deriving(Row)` which maps through Map<String, String> -- check how existing code handles Int fields. SAFE PATH: Cast event_count::text in SQL, use `Map.get(row, "event_count")` and rely on the fact that the Issue struct is constructed manually (not via from_row). Construct Issue struct manually in the map callback.

8. `pub fn check_volume_spikes(pool :: PoolHandle) -> Int!String` -- ISSUE-03
   SQL: `UPDATE issues SET status = 'unresolved' WHERE status = 'archived' AND id IN (SELECT i.id FROM issues i JOIN events e ON e.issue_id = i.id AND e.received_at > now() - interval '1 hour' WHERE i.status = 'archived' GROUP BY i.id HAVING count(*) > GREATEST(10, (SELECT count(*) FROM events e2 WHERE e2.issue_id = i.id AND e2.received_at > now() - interval '7 days') / 168 * 10))`
   Uses Pool.execute, returns number of escalated issues.
   The HAVING clause: recent hourly count > MAX(10, 10x average hourly rate over 7 days). The WHERE status='archived' naturally prevents re-escalation after the first escalation (research Pitfall 5).

**Extend `mesher/ingestion/routes.mpl`** with issue management HTTP route handlers.

Add imports at top:
```
from Storage.Queries import resolve_issue, archive_issue, unresolve_issue, assign_issue, discard_issue, delete_issue, list_issues_by_status
```

Add helper functions and route handlers. Each handler follows the existing pattern: get registry -> get pool -> execute query -> return JSON response. Use POST for all state transitions (HTTP.on_put and HTTP.on_delete exist in the runtime, but POST routes with action-in-path is simpler and consistent with the existing pattern).

Route handlers (all pub):

1. `pub fn handle_list_issues(request)` -- GET /api/v1/projects/:project_id/issues?status=unresolved
   Extract project_id from Request.param(request, "project_id"). Extract status from query string -- since Mesh may not have query string parsing, default to "unresolved" status. Call list_issues_by_status. Return JSON array of issues using Issue.to_json (deriving(Json) provides this). Actually, deriving(Json) provides to_json, but cross-module concerns may apply. SAFE APPROACH: Build the JSON response manually using string concatenation for the list of issues, OR use Json.encode on each Issue if to_json is available.

   Actually, Issue derives Json, so `Issue.to_json(issue)` should work. But cross-module to_json -- check if this is an issue. Since we construct Issue structs in routes.mpl and Issue is imported, to_json should be available. If not, fall back to manual JSON string building.

   SIMPLEST: Return issues as a JSON array built with string concatenation:
   ```
   fn issue_to_json_str(issue :: Issue) -> String do
     "{\"id\":\"" <> issue.id <> "\",\"title\":\"" <> issue.title <> "\",\"level\":\"" <> issue.level <> "\",\"status\":\"" <> issue.status <> "\",\"event_count\":" <> issue.event_count <> ",\"first_seen\":\"" <> issue.first_seen <> "\",\"last_seen\":\"" <> issue.last_seen <> "\",\"assigned_to\":\"" <> issue.assigned_to <> "\"}"
   end
   ```
   NOTE: event_count is Int in the Issue struct, but since we construct it from Map.get (which returns String), we'll keep it as String for JSON building. Actually we need to look at the Issue struct: `event_count :: Int`. If we construct Issue with `event_count: Map.get(row, "event_count")`, that's String being assigned to Int, which will cause a type error. We MUST convert: no, wait -- Map.get returns String, but the struct field is Int. Type checker would flag this.

   RESOLUTION for Issue struct construction: Since Issue has `event_count :: Int` and Map.get returns String, we need String.to_int. But String.to_int returns `Int!String` (Result). So: `String.to_int(Map.get(row, "event_count"))` -- but we need to unwrap it. Use a helper:
   ```
   fn parse_event_count(s :: String) -> Int do
     let result = String.to_int(s)
     case result do
       Ok(n) -> n
       Err(_) -> 0
     end
   end
   ```
   Then in list_issues_by_status, construct Issue with `event_count: parse_event_count(Map.get(row, "event_count"))`.

   Wait -- this parse helper and Issue construction is in queries.mpl, not routes.mpl. Move the list_issues_by_status implementation to include the parse helper. Actually, looking at the Issue struct more carefully -- it derives Row. The `deriving(Row)` macro should handle Int parsing automatically via `from_row`. But decision [87-01] says "Row structs use all-String fields" -- yet Issue has `event_count :: Int`. This might be a latent issue. CHECK: Does `deriving(Row)` handle Int fields? Per [87-01] and the from_row documentation, from_row maps Int fields via parse. So `Pool.query_as` or manual `from_row` should work.

   SAFEST APPROACH for queries.mpl: Construct Issue manually with parse_event_count helper for the Int field. Keep event_count::text in SQL cast. This avoids depending on untested from_row Int handling.

2. `pub fn handle_resolve_issue(request)` -- POST /api/v1/issues/:id/resolve
   Extract id from Request.param. Call resolve_issue(pool, id). Return 200 OK or 404.

3. `pub fn handle_archive_issue(request)` -- POST /api/v1/issues/:id/archive
   Same pattern, call archive_issue.

4. `pub fn handle_unresolve_issue(request)` -- POST /api/v1/issues/:id/unresolve
   Same pattern, call unresolve_issue.

5. `pub fn handle_assign_issue(request)` -- POST /api/v1/issues/:id/assign
   Extract user_id from request body (parse JSON body, get "user_id" field). Call assign_issue.

6. `pub fn handle_discard_issue(request)` -- POST /api/v1/issues/:id/discard
   Call discard_issue.

7. `pub fn handle_delete_issue(request)` -- POST /api/v1/issues/:id/delete
   Call delete_issue. Returns 200 on success.

For all handlers, follow the pattern:
```
pub fn handle_resolve_issue(request) do
  let reg_pid = Process.whereis("mesher_registry")
  let pool = PipelineRegistry.get_pool(reg_pid)
  let issue_id = Request.param(request, "id")
  let result = resolve_issue(pool, issue_id)
  case result do
    Ok(n) -> HTTP.response(200, "{\"status\":\"ok\",\"affected\":" <> String.from(n) <> "}")
    Err(e) -> HTTP.response(500, "{\"error\":\"" <> e <> "\"}")
  end
end
```

For handle_assign_issue, parse request body to extract user_id:
```
pub fn handle_assign_issue(request) do
  let reg_pid = Process.whereis("mesher_registry")
  let pool = PipelineRegistry.get_pool(reg_pid)
  let issue_id = Request.param(request, "id")
  let body = Request.body(request)
  # Extract user_id from JSON body using PostgreSQL (same technique as extract_event_fields)
  let rows_result = Pool.query(pool, "SELECT COALESCE($1::jsonb->>'user_id', '') AS user_id", [body])
  case rows_result do
    Err(e) -> HTTP.response(400, "{\"error\":\"invalid json\"}")
    Ok(rows) -> assign_from_rows(pool, issue_id, rows)
  end
end

fn assign_from_rows(pool :: PoolHandle, issue_id :: String, rows) do
  if List.length(rows) > 0 do
    let user_id = Map.get(List.head(rows), "user_id")
    let result = assign_issue(pool, issue_id, user_id)
    case result do
      Ok(n) -> HTTP.response(200, "{\"status\":\"ok\"}")
      Err(e) -> HTTP.response(500, "{\"error\":\"" <> e <> "\"}")
    end
  else
    HTTP.response(400, "{\"error\":\"invalid body\"}")
  end
end
```

IMPORTANT: Each handler must be a pub function with a single `request` parameter (HTTP routing convention). Add import for PipelineRegistry at top of routes.mpl (already imported).

Add `from Storage.Queries import ...` with all the new query functions. Extend the existing import line.
  </action>
  <verify>
Run `cd /Users/sn0w/Documents/dev/snow && cargo build --release 2>&1 | tail -30` to verify compilation. Check that all route handler functions are pub and have the correct single-parameter signature.
  </verify>
  <done>
Issue management queries exist in queries.mpl (resolve, archive, unresolve, assign, discard, delete, list_by_status, check_volume_spikes). Route handlers exist in routes.mpl for all issue management operations. All handlers follow the registry lookup pattern.
  </done>
</task>

<task type="auto">
  <name>Task 2: Spike detection actor and main.mpl route registration</name>
  <files>mesher/ingestion/pipeline.mpl, mesher/main.mpl</files>
  <action>
**Extend `mesher/ingestion/pipeline.mpl`** -- Add spike detection ticker actor.

Add import: `from Storage.Queries import check_volume_spikes`

Add spike checker actor following the established Timer.sleep + recursive call pattern (decision [87-02], [88-05]):
```mesh
# Periodic spike detection actor -- checks archived issues for volume spikes.
# Runs every 5 minutes (300000ms). If an archived issue has a sudden burst of events
# (>10x average hourly rate), it's auto-escalated to 'unresolved' (ISSUE-03).
# Uses Timer.sleep + recursive call pattern (established in flush_ticker, health_checker).
actor spike_checker(pool :: PoolHandle) do
  Timer.sleep(300000)
  let result = check_volume_spikes(pool)
  case result do
    Ok(n) ->
      if n > 0 do
        println("[Mesher] Spike checker: escalated " <> String.from(n) <> " archived issues")
      else
        0
      end
    Err(e) -> println("[Mesher] Spike checker error: " <> e)
  end
  spike_checker(pool)
end
```

Spawn the spike_checker in `start_pipeline`, after the health checker:
```mesh
# Spawn spike detection checker (5 minute interval)
let _ = spawn(spike_checker, pool)
println("[Mesher] Spike checker started (5 min interval)")
```

**Extend `mesher/main.mpl`** -- Register issue management HTTP routes.

Add imports at top for the new route handlers:
```
from Ingestion.Routes import handle_event, handle_bulk, handle_list_issues, handle_resolve_issue, handle_archive_issue, handle_unresolve_issue, handle_assign_issue, handle_discard_issue, handle_delete_issue
```

In `start_services`, extend the router registration section after the existing event routes:
```mesh
# Issue management routes
let r = HTTP.on_get(r, "/api/v1/projects/:project_id/issues", handle_list_issues)
let r = HTTP.on_post(r, "/api/v1/issues/:id/resolve", handle_resolve_issue)
let r = HTTP.on_post(r, "/api/v1/issues/:id/archive", handle_archive_issue)
let r = HTTP.on_post(r, "/api/v1/issues/:id/unresolve", handle_unresolve_issue)
let r = HTTP.on_post(r, "/api/v1/issues/:id/assign", handle_assign_issue)
let r = HTTP.on_post(r, "/api/v1/issues/:id/discard", handle_discard_issue)
let r = HTTP.on_post(r, "/api/v1/issues/:id/delete", handle_delete_issue)
```

All routes use POST for state transitions (consistent with existing pattern -- POST /api/v1/events). The list endpoint uses GET. Path parameters use `:id` and `:project_id` which are extracted via `Request.param` in the handlers.

Verify the existing imports in main.mpl don't conflict with the new ones. The current import from Ingestion.Routes is: `from Ingestion.Routes import handle_event, handle_bulk`. Extend it to include all new handlers.
  </action>
  <verify>
1. Run `cd /Users/sn0w/Documents/dev/snow && cargo build --release 2>&1 | tail -30` -- must compile successfully.
2. Verify spike_checker actor is spawned in start_pipeline by reading the file.
3. Verify all 7 issue management routes are registered in main.mpl by reading the router section.
4. Verify spike detection SQL in check_volume_spikes addresses the WHERE status='archived' guard against re-escalation.
  </verify>
  <done>
Spike detection actor exists in pipeline.mpl, spawned during pipeline startup with 5-minute interval. All issue management HTTP routes registered in main.mpl (list, resolve, archive, unresolve, assign, discard, delete). Project compiles. Requirements ISSUE-01 (state transitions), ISSUE-03 (auto-escalation), ISSUE-04 (assignment), ISSUE-05 (delete/discard) satisfied.
  </done>
</task>

</tasks>

<verification>
1. Compile: `cd /Users/sn0w/Documents/dev/snow && cargo build --release` must succeed.
2. Route count: 9 HTTP routes registered (2 existing event routes + 7 new issue routes).
3. Query count: 8 new functions in queries.mpl (upsert_issue, is_issue_discarded, extract_event_fields from Plan 01, plus resolve, archive, unresolve, assign, discard, delete, list_by_status, check_volume_spikes).
4. Actor count: spike_checker actor spawned in pipeline startup.
5. State machine: resolve -> 'resolved', archive -> 'archived', unresolve -> 'unresolved', discard -> 'discarded'. Regression detection flips 'resolved' -> 'unresolved' on new event. Spike escalation flips 'archived' -> 'unresolved'.
</verification>

<success_criteria>
- queries.mpl has all issue management queries: resolve, archive, unresolve, assign, discard, delete, list_by_status, check_volume_spikes
- routes.mpl has HTTP handlers for all 7 issue management endpoints
- pipeline.mpl has spike_checker actor with 5-minute Timer.sleep interval
- main.mpl registers all issue management routes and spike checker is spawned
- State machine is complete: unresolved <-> resolved, any -> archived, archived -> unresolved (spike), resolved -> unresolved (regression), any -> discarded
- Project compiles with `cargo build --release`
</success_criteria>

<output>
After completion, create `.planning/phases/89-error-grouping-issue-lifecycle/89-02-SUMMARY.md`
</output>
