---
phase: 66-remote-links-monitors-failure-handling
plan: 02
type: execute
wave: 2
depends_on: ["66-01"]
files_modified:
  - crates/snow-rt/src/dist/node.rs
  - crates/snow-rt/src/actor/mod.rs
  - crates/snow-rt/src/lib.rs
autonomous: true

must_haves:
  truths:
    - "NodeState has a node_monitors registry tracking which processes monitor which nodes"
    - "snow_node_monitor registers the calling process to receive nodedown/nodeup for a given node"
    - "When a node disconnects, all processes with monitors on that node receive :nodedown messages"
    - "When a node disconnects, all local processes with remote links to that node get :noconnection exit signals"
    - "When a node disconnects, all local processes with remote monitors on that node get DOWN(noconnection) messages"
    - "When a node connects, processes monitoring that node receive :nodeup messages"
  artifacts:
    - path: "crates/snow-rt/src/dist/node.rs"
      provides: "node_monitors field on NodeState, handle_node_disconnect, nodedown/nodeup delivery, DIST_MONITOR/DEMONITOR/MONITOR_EXIT wire handlers"
      contains: "handle_node_disconnect"
    - path: "crates/snow-rt/src/actor/mod.rs"
      provides: "snow_node_monitor extern C API"
      contains: "snow_node_monitor"
  key_links:
    - from: "crates/snow-rt/src/dist/node.rs"
      to: "crates/snow-rt/src/actor/link.rs"
      via: "encode_exit_signal, encode_down_signal, DOWN_SIGNAL_TAG for disconnect signal synthesis"
      pattern: "link::encode_exit_signal|link::encode_down_signal|link::DOWN_SIGNAL_TAG"
    - from: "crates/snow-rt/src/dist/node.rs cleanup_session"
      to: "crates/snow-rt/src/dist/node.rs handle_node_disconnect"
      via: "cleanup_session calls handle_node_disconnect after removing session"
      pattern: "handle_node_disconnect"
---

<objective>
Add node monitoring API and connection loss propagation: node_monitors registry on NodeState, snow_node_monitor API, handle_node_disconnect function that fires :noconnection exit signals and DOWN messages, cleanup_session wiring, DIST_MONITOR/DEMONITOR/MONITOR_EXIT reader loop handlers, and nodeup delivery on connection.

Purpose: When a node disconnects, all local processes with remote links or monitors learn about it immediately. This is the core of distributed fault tolerance -- "no silent failures."
Output: Complete node-level failure detection and propagation.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/66-remote-links-monitors-failure-handling/66-RESEARCH.md
@.planning/phases/66-remote-links-monitors-failure-handling/66-01-SUMMARY.md

@crates/snow-rt/src/dist/node.rs
@crates/snow-rt/src/actor/mod.rs
@crates/snow-rt/src/actor/link.rs
@crates/snow-rt/src/actor/process.rs
@crates/snow-rt/src/actor/scheduler.rs
@crates/snow-rt/src/lib.rs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add node_monitors to NodeState, snow_node_monitor API, and handle_node_disconnect</name>
  <files>crates/snow-rt/src/dist/node.rs, crates/snow-rt/src/actor/mod.rs, crates/snow-rt/src/lib.rs</files>
  <action>
In `dist/node.rs`:

1. Add `node_monitors` field to `NodeState` struct:
   ```rust
   /// Processes monitoring specific nodes for :nodedown/:nodeup.
   /// Maps node_name -> list of (monitoring_pid, is_once) pairs.
   pub node_monitors: RwLock<FxHashMap<String, Vec<(ProcessId, bool)>>>,
   ```
   Initialize it in `snow_node_start` where NodeState is constructed: `node_monitors: RwLock::new(FxHashMap::default())`.

2. Add three new wire message tag constants after DIST_PEER_LIST (0x12):
   ```rust
   pub(crate) const DIST_MONITOR: u8 = 0x16;
   pub(crate) const DIST_DEMONITOR: u8 = 0x17;
   pub(crate) const DIST_MONITOR_EXIT: u8 = 0x18;
   ```

3. Add `handle_node_disconnect` function (the CORE of Phase 66):
   - Takes `node_name: &str` and `node_id: u16`
   - Gets the global scheduler via `crate::actor::GLOBAL_SCHEDULER.get()`
   - Creates `noconnection = ExitReason::Noconnection`
   - **Two-phase approach to avoid deadlocks:**
     - Phase 1 (collect): Under process table READ lock, iterate all processes. For each process:
       - Collect remote links to the disconnected node (where `linked_pid.node_id() == node_id` from the links HashSet) into a `Vec<(ProcessId, Vec<ProcessId>)>` mapping local_pid -> remote_pids_to_unlink
       - Collect remote monitors to the disconnected node (where `monitored_pid.node_id() == node_id` from the monitors FxHashMap) into a `Vec<(ProcessId, Vec<(u64, ProcessId)>)>` mapping local_pid -> Vec<(monitor_ref, monitored_pid)>
     - Drop the process table read lock
     - Phase 2 (execute): For each collected entry, re-acquire process arc and deliver signals:
       - For remote links: remove the remote PIDs from links, then apply exit signal logic (if trap_exit: deliver as message with EXIT_SIGNAL_TAG; else: set Exited(Linked(remote_pid, Noconnection)))
       - For remote monitors: remove from monitors map, deliver DOWN message with Noconnection reason using encode_down_signal + DOWN_SIGNAL_TAG
   - After process signals, deliver :nodedown to node monitors:
     - Read node_monitors for the disconnecting node_name
     - For each watcher PID, deliver a nodedown message. Use a dedicated nodedown message format:
       - Define `NODEDOWN_TAG: u64 = u64::MAX - 2` constant (new reserved tag)
       - Encode: `[node_name_bytes]` as the payload with type_tag = NODEDOWN_TAG
       - Deliver via `local_send`-style push to the watcher's mailbox
     - Remove "once" monitors (retain those with `is_once == false`)

4. Modify `cleanup_session` to call `handle_node_disconnect` after removing the session:
   ```rust
   fn cleanup_session(remote_name: &str) {
       if let Some(state) = NODE_STATE.get() {
           let removed = {
               let mut sessions = state.sessions.write();
               sessions.remove(remote_name)
           };
           if let Some(session) = removed {
               let node_id = session.node_id;
               let mut id_map = state.node_id_map.write();
               id_map.remove(&node_id);
               drop(id_map);
               // Phase 66: Fire all failure signals for disconnected node.
               handle_node_disconnect(remote_name, node_id);
           }
       }
   }
   ```

5. Add `handle_node_connect` function for :nodeup delivery:
   - Called from `register_session` at the end (after session is fully registered)
   - Reads `node_monitors` for the connecting node_name
   - For each watcher PID, delivers a nodeup message:
     - Define `NODEUP_TAG: u64 = u64::MAX - 3` constant (new reserved tag)
     - Encode: `[node_name_bytes]` as the payload with type_tag = NODEUP_TAG
   - Call this from the end of `register_session` (after session and id_map are set up)

6. Add DIST_MONITOR, DIST_DEMONITOR, DIST_MONITOR_EXIT handlers to `reader_loop_session`:
   - `DIST_MONITOR` (0x16): Wire format `[tag][u64 from_pid][u64 to_pid][u64 ref]`
     - from_pid is the remote monitoring process, to_pid is the local target
     - Look up the local target process. If alive, add `(ref, from_pid)` to its `monitored_by`. If dead/not found, immediately send back DIST_MONITOR_EXIT with "noproc" reason.
   - `DIST_DEMONITOR` (0x17): Wire format `[tag][u64 from_pid][u64 to_pid][u64 ref]`
     - Remove `ref` from the local target's `monitored_by`
   - `DIST_MONITOR_EXIT` (0x18): Wire format `[tag][u64 monitored_pid][u64 monitoring_pid][u64 ref][reason_bytes]`
     - Look up the local monitoring process. Remove `ref` from its `monitors`. Deliver DOWN message.

7. Add helper `send_dist_monitor_exit` for sending DIST_MONITOR_EXIT back to a remote node:
   ```rust
   fn send_dist_monitor_exit(session: &Arc<NodeSession>, monitored_pid: ProcessId, monitoring_pid: ProcessId, monitor_ref: u64, reason: &ExitReason) {
       let mut payload = Vec::with_capacity(1 + 8 + 8 + 8 + 16);
       payload.push(DIST_MONITOR_EXIT);
       payload.extend_from_slice(&monitored_pid.as_u64().to_le_bytes());
       payload.extend_from_slice(&monitoring_pid.as_u64().to_le_bytes());
       payload.extend_from_slice(&monitor_ref.to_le_bytes());
       crate::actor::link::encode_reason(&mut payload, reason);
       let mut stream = session.stream.lock().unwrap();
       let _ = write_msg(&mut *stream, &payload);
   }
   ```

In `actor/mod.rs`:
1. Add `snow_node_monitor` extern "C" function:
   ```rust
   #[no_mangle]
   pub extern "C" fn snow_node_monitor(node_ptr: *const u8, node_len: u64) -> u64 {
       // Get current PID, return 1 on failure
       // Parse node_name from (node_ptr, node_len)
       // Get NODE_STATE, return 1 if not started
       // Insert (my_pid, false) into node_monitors entry for node_name
       // Return 0 on success
   }
   ```

2. Extend `snow_process_monitor` to handle remote targets: if target is remote (`node_id() != 0`), after recording locally in `monitors`, send a DIST_MONITOR wire message to the remote node. Add a helper function `send_dist_monitor(from_pid, to_pid, monitor_ref)` that looks up the session and sends the wire message. If the session is not found, immediately deliver DOWN(noconnection) to the caller.

In `lib.rs`:
   Add `snow_node_monitor` to the `pub use dist::node::{...}` re-export.
   Also export `NODEDOWN_TAG` and `NODEUP_TAG` if they are in dist/node.rs, or keep them there as pub(crate).
  </action>
  <verify>Run `cargo build -p snow-rt 2>&1 | tail -20` -- no errors. Run `cargo test -p snow-rt 2>&1 | tail -10` -- all tests pass.</verify>
  <done>NodeState has node_monitors registry. snow_node_monitor registers for nodedown/nodeup. cleanup_session triggers handle_node_disconnect which fires :noconnection for remote links, DOWN(noconnection) for remote monitors, and :nodedown to node monitors. register_session triggers :nodeup. DIST_MONITOR/DEMONITOR/MONITOR_EXIT handled in reader loop. Remote process monitor sends DIST_MONITOR wire message.</done>
</task>

</tasks>

<verification>
```bash
# Build succeeds
cargo build -p snow-rt 2>&1 | tail -5

# All tests pass
cargo test -p snow-rt 2>&1 | tail -10

# Verify key functions exist
grep -n "handle_node_disconnect" crates/snow-rt/src/dist/node.rs
grep -n "handle_node_connect" crates/snow-rt/src/dist/node.rs
grep -n "snow_node_monitor" crates/snow-rt/src/actor/mod.rs
grep -n "DIST_MONITOR" crates/snow-rt/src/dist/node.rs
grep -n "NODEDOWN_TAG" crates/snow-rt/src/dist/node.rs
grep -n "node_monitors" crates/snow-rt/src/dist/node.rs
```
</verification>

<success_criteria>
1. NodeState has `node_monitors: RwLock<FxHashMap<String, Vec<(ProcessId, bool)>>>` field
2. handle_node_disconnect scans all processes for remote links/monitors to the disconnected node
3. Remote links get :noconnection exit signals (respecting trap_exit)
4. Remote monitors get DOWN(noconnection) messages
5. Node monitors get :nodedown messages (NODEDOWN_TAG)
6. register_session delivers :nodeup messages (NODEUP_TAG) to node monitors
7. DIST_MONITOR/DEMONITOR/MONITOR_EXIT wire messages handled in reader loop
8. DIST_MONITOR on dead process immediately sends DIST_MONITOR_EXIT back
9. Remote snow_process_monitor sends DIST_MONITOR wire message
10. All existing tests pass with zero regressions
</success_criteria>

<output>
After completion, create `.planning/phases/66-remote-links-monitors-failure-handling/66-02-SUMMARY.md`
</output>
