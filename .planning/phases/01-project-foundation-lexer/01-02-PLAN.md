---
phase: 01-project-foundation-lexer
plan: 02
type: execute
wave: 2
depends_on: ["01-01"]
files_modified:
  - crates/snow-lexer/src/lib.rs
  - crates/snow-lexer/src/cursor.rs
  - crates/snow-lexer/tests/lexer_tests.rs
  - tests/fixtures/keywords.snow
  - tests/fixtures/operators.snow
  - tests/fixtures/numbers.snow
  - tests/fixtures/identifiers.snow
autonomous: true

must_haves:
  truths:
    - "Lexer tokenizes all 37 keywords into distinct TokenKind variants"
    - "Lexer tokenizes all operators including multi-character operators (|>, ==, ->, =>, ::, <=, >=, !=, &&, ||, ++, .., <>)"
    - "Lexer tokenizes integer literals in all bases (decimal, hex, binary, octal) with underscore separators"
    - "Lexer tokenizes float literals with basic and scientific notation"
    - "Lexer tokenizes identifiers and distinguishes them from keywords"
    - "Every token carries a correct byte-offset Span"
    - "Simple double-quoted strings (without interpolation) tokenize correctly"
  artifacts:
    - path: "crates/snow-lexer/src/cursor.rs"
      provides: "Byte-level source iteration with peek, advance, position tracking"
      contains: "pub struct Cursor"
    - path: "crates/snow-lexer/src/lib.rs"
      provides: "Main Lexer struct implementing Iterator<Item = Token>"
      contains: "impl Iterator for Lexer"
    - path: "crates/snow-lexer/tests/lexer_tests.rs"
      provides: "Insta snapshot tests for keywords, operators, numbers, identifiers"
      min_lines: 50
  key_links:
    - from: "crates/snow-lexer/src/lib.rs"
      to: "crates/snow-common/src/token.rs"
      via: "use snow_common::token::{Token, TokenKind}"
      pattern: "use snow_common"
    - from: "crates/snow-lexer/src/lib.rs"
      to: "crates/snow-lexer/src/cursor.rs"
      via: "mod cursor; use cursor::Cursor"
      pattern: "mod cursor"
    - from: "crates/snow-lexer/tests/lexer_tests.rs"
      to: "crates/snow-lexer/src/lib.rs"
      via: "use snow_lexer::Lexer"
      pattern: "use snow_lexer"
---

<objective>
Implement the core lexer with Cursor-based source iteration, tokenization of keywords, operators, numbers, identifiers, simple strings, and line comments. This plan builds the lexer's main loop and all "simple" token handling -- everything except string interpolation, block comments, newline-as-terminator logic, and error recovery (which are Plan 03).

Purpose: The core lexer handles ~80% of all tokens Snow programs will contain. Getting the cursor, main loop, and simple tokens right establishes the pattern that Plan 03 extends for complex cases.

Output: A working `Lexer` struct that implements `Iterator<Item = Token>`, a `Cursor` for byte-level iteration, and insta snapshot tests proving correct tokenization of keywords, operators, numbers, identifiers, and simple strings.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-project-foundation-lexer/01-CONTEXT.md
@.planning/phases/01-project-foundation-lexer/01-RESEARCH.md
@.planning/phases/01-project-foundation-lexer/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Cursor and core Lexer struct with main tokenization loop</name>
  <files>
    crates/snow-lexer/src/cursor.rs
    crates/snow-lexer/src/lib.rs
  </files>
  <action>
    **`crates/snow-lexer/src/cursor.rs`** -- Low-level byte/char iteration:

    ```rust
    pub struct Cursor<'src> {
        source: &'src str,
        pos: u32,          // current byte position
        chars: std::str::Chars<'src>,
    }
    ```

    Methods:
    - `Cursor::new(source: &str) -> Cursor` -- initialize at position 0
    - `peek(&self) -> Option<char>` -- look at current char without consuming (use `self.chars.clone().next()`)
    - `peek_next(&self) -> Option<char>` -- look at char after current (clone chars, skip one, take next)
    - `advance(&mut self) -> Option<char>` -- consume current char, advance position by char's UTF-8 byte length
    - `pos(&self) -> u32` -- current byte position
    - `is_eof(&self) -> bool` -- no more chars
    - `eat_while(&mut self, predicate: impl Fn(char) -> bool)` -- advance while predicate holds
    - `slice(&self, start: u32, end: u32) -> &str` -- extract slice of source text by byte offsets

    IMPORTANT: Position tracking must account for multi-byte UTF-8 characters. Use `char::len_utf8()` when advancing.

    **`crates/snow-lexer/src/lib.rs`** -- Main Lexer:

    Replace the stub with a real implementation. The Lexer wraps a Cursor and implements `Iterator<Item = Token>`.

    ```rust
    pub struct Lexer<'src> {
        cursor: Cursor<'src>,
        source: &'src str,
    }
    ```

    Public API:
    - `Lexer::new(source: &str) -> Lexer` -- create lexer for source text
    - `Lexer::tokenize(source: &str) -> Vec<Token>` -- convenience: collect all tokens into Vec (includes Eof)
    - Implement `Iterator for Lexer` where `next()` returns `Some(Token)` until `Eof`, then `None`

    Main loop in `next_token(&mut self) -> Token`:
    1. Skip whitespace (spaces, tabs, carriage returns -- NOT newlines, those are handled separately for now, but in this plan just skip them too as regular whitespace; Plan 03 adds newline-as-terminator logic)
    2. Record `start` position
    3. Match on current character to determine token:

    **Single-char tokens (no ambiguity):**
    `(`, `)`, `[`, `]`, `{`, `}`, `,`, `;`

    **Multi-char operators (longest match first):**
    - `=` -> check next: `=` -> `EqEq`, `>` -> `FatArrow`, else `Eq`
    - `!` -> check next: `=` -> `NotEq`, else `Bang`
    - `<` -> check next: `=` -> `LtEq`, `>` -> `Diamond`, else `Lt`
    - `>` -> check next: `=` -> `GtEq`, else `Gt`
    - `&` -> check next: `&` -> `AmpAmp`, else emit Error token (single `&` is not valid Snow)
    - `|` -> check next: `|` -> `PipePipe`, `>` -> `Pipe` (pipe operator), else emit Error token
    - `+` -> check next: `+` -> `PlusPlus`, else `Plus`
    - `-` -> check next: `>` -> `Arrow`, else `Minus`
    - `:` -> check next: `:` -> `ColonColon`, else `Colon`
    - `.` -> check next: `.` -> `DotDot`, else `Dot`
    - `*` -> `Star`
    - `/` -> `Slash`
    - `%` -> `Percent`

    **Comments (starting with `#`):**
    - `#` followed by `#` followed by `!` -> scan to end of line -> `ModuleDocComment` (content after `##!`)
    - `#` followed by `#` (but not `!`) -> scan to end of line -> `DocComment` (content after `##`)
    - `#` followed by `=` -> for now, emit a placeholder/simple handling (Plan 03 implements full nestable block comments)
    - `#` otherwise -> scan to end of line -> `Comment`

    **Number literals:**
    - Starts with digit `0-9`:
      - `0x` or `0X` -> hex: eat hex digits and underscores -> `IntLiteral`
      - `0b` or `0B` -> binary: eat `0`, `1`, and underscores -> `IntLiteral`
      - `0o` or `0O` -> octal: eat `0-7` and underscores -> `IntLiteral`
      - Otherwise -> decimal digits with underscores
        - If followed by `.` and then a digit (NOT `..` range operator!) -> float: eat digits, optional `e`/`E` with optional `+`/`-` -> `FloatLiteral`
        - If followed by `e`/`E` -> scientific float -> `FloatLiteral`
        - Else -> `IntLiteral`

    **String literals (simple -- no interpolation yet):**
    - `"` -> for this plan, implement basic string scanning:
      - Check for `"""` (triple-quote) -> scan until closing `"""` -> emit as `StringStart`, `StringContent`, `StringEnd` sequence
      - Otherwise single `"` -> scan until closing `"`, handling escape sequences (`\\`, `\"`, `\n`, `\t`, `\r`, `\0`) -> emit as `StringStart`, `StringContent`, `StringEnd` sequence
      - When encountering `${` inside a string, for NOW just include it in StringContent. Plan 03 adds proper interpolation handling.
      - Unterminated string (hit EOF) -> emit `Error` token

    **Identifiers and keywords:**
    - Starts with alphabetic char or `_`:
      - Eat alphanumeric chars and underscores
      - Check result against `keyword_from_str()` from snow-common
      - If keyword -> return keyword TokenKind
      - If `true` -> `True`, `false` -> `False`, `nil` -> `Nil` (these are in the keyword list)
      - Else -> `Ident`

    **EOF:**
    - No more input -> `Eof`

    **Unknown characters:**
    - Emit `Error` token with the character, advance past it (panic-mode recovery)

    Make the module structure: `mod cursor;` in lib.rs, and re-export `Lexer` as the public API.
  </action>
  <verify>
    Run `cargo build -p snow-lexer` -- must compile.
    Run `cargo test -p snow-lexer` -- must pass (even if no snapshot tests yet).
    Manually verify the Iterator impl by adding a temporary `#[test]` in lib.rs that tokenizes `"let x = 42"` and checks it produces `[Let, Ident, Eq, IntLiteral, Eof]` (remove or keep this test as desired).
  </verify>
  <done>
    Cursor provides byte-level iteration with peek/advance/position tracking. Lexer implements Iterator<Item = Token> with a main loop that handles all single-char tokens, multi-char operators, line comments, doc comments, number literals (all bases with underscores, floats with scientific notation), simple strings, identifiers, keywords, and error recovery for unknown chars. `cargo build` succeeds.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add insta snapshot tests for core token types</name>
  <files>
    crates/snow-lexer/tests/lexer_tests.rs
    tests/fixtures/keywords.snow
    tests/fixtures/operators.snow
    tests/fixtures/numbers.snow
    tests/fixtures/identifiers.snow
  </files>
  <action>
    Create test fixtures as `.snow` files in `tests/fixtures/` at workspace root. These are plain text files containing Snow source code designed to exercise specific token categories.

    **`tests/fixtures/keywords.snow`:**
    ```
    fn let struct trait module import if else case match do end
    receive spawn supervisor send link monitor trap
    cond with after alias and or not in for
    pub impl type def return self when where
    true false nil
    ```
    One line with all 37 keywords plus true/false/nil.

    **`tests/fixtures/operators.snow`:**
    ```
    + - * / %
    == != < > <= >=
    && || !
    |> .. <> ++
    = -> => ::
    ```
    Grouped by category, one group per line.

    **`tests/fixtures/numbers.snow`:**
    ```
    42
    1_000_000
    0xFF
    0xff_ff
    0b1010
    0b1111_0000
    0o777
    3.14
    1.0e10
    2.5e-3
    100_000.50
    ```

    **`tests/fixtures/identifiers.snow`:**
    ```
    x foo bar_baz _private camelCase PascalCase x1 _
    ```

    **`crates/snow-lexer/tests/lexer_tests.rs`:**

    Integration test file using insta for snapshot testing.

    ```rust
    use snow_lexer::Lexer;
    use insta::assert_yaml_snapshot;
    ```

    Create a helper function `tokenize_snapshot(source: &str) -> Vec<TokenSnapshot>` that maps tokens to a serializable format showing kind and the source text slice (using the span to extract from source). This makes snapshots human-readable:
    ```rust
    #[derive(serde::Serialize)]
    struct TokenSnapshot {
        kind: String,       // Debug format of TokenKind
        text: String,       // source[span.start..span.end]
        span: (u32, u32),   // (start, end) byte offsets
    }
    ```

    Tests to write:
    1. `test_keywords` -- tokenize `keywords.snow`, snapshot the token stream. Verify all 37 keywords produce correct TokenKind variants.
    2. `test_operators` -- tokenize `operators.snow`, snapshot. Verify all operator tokens.
    3. `test_numbers` -- tokenize `numbers.snow`, snapshot. Verify IntLiteral vs FloatLiteral distinction, all number bases.
    4. `test_identifiers` -- tokenize `identifiers.snow`, snapshot. Verify all produce `Ident` (not keywords).
    5. `test_simple_string` -- inline source `"hello world"`, verify StringStart + StringContent + StringEnd sequence.
    6. `test_line_comment` -- inline source `# this is a comment`, verify Comment token with content.
    7. `test_doc_comment` -- inline source `## this is a doc comment`, verify DocComment.
    8. `test_module_doc_comment` -- inline source `##! module doc`, verify ModuleDocComment.
    9. `test_mixed_expression` -- inline source `let result = add(1, 2) |> multiply(3)`, verify full token stream.
    10. `test_spans_accurate` -- tokenize `let x = 42`, verify each token's span byte offsets are correct (Let: 0-3, Ident: 4-5, Eq: 6-7, IntLiteral: 8-10).

    Run `cargo insta test --accept -p snow-lexer` to generate and accept snapshot files. Then run `cargo test -p snow-lexer` to confirm they pass.

    IMPORTANT: If any snapshot test fails because the lexer output doesn't match expected, fix the lexer (in lib.rs or cursor.rs) to produce correct output. The tests define correctness.
  </action>
  <verify>
    Run `cargo test -p snow-lexer` -- all 10+ snapshot tests pass.
    Run `cargo insta test -p snow-lexer` -- no pending snapshots (all accepted).
    Verify snapshot files exist in `crates/snow-lexer/tests/snapshots/` directory.
    Inspect at least one snapshot file to confirm it contains human-readable YAML with token kinds and source text slices.
  </verify>
  <done>
    Insta snapshot tests cover all core token categories: 37 keywords, all operators, all number formats (decimal, hex, binary, octal, floats, scientific notation, underscore separators), identifiers, simple strings, line comments, doc comments, module doc comments, and a mixed expression. All tests pass. Snapshot files provide human-readable YAML of token streams for easy review.
  </done>
</task>

</tasks>

<verification>
1. `cargo test -p snow-lexer` passes with 10+ snapshot tests
2. Lexer correctly tokenizes all 37 keywords (verified by snapshot)
3. Multi-character operators tokenized correctly: `|>`, `==`, `->`, `=>`, `::`, `<=`, `>=`, `!=`, `&&`, `||`, `++`, `..`, `<>`
4. Number literals: `42` -> IntLiteral, `3.14` -> FloatLiteral, `0xFF` -> IntLiteral, `1.0e10` -> FloatLiteral
5. Identifiers distinguished from keywords
6. Span byte offsets are accurate for every token
7. Simple strings tokenize as StringStart + StringContent + StringEnd sequence
8. Comments tokenize correctly (line, doc, module doc)
</verification>

<success_criteria>
- Lexer implements Iterator<Item = Token> with correct tokenization
- Cursor handles UTF-8 source text with byte-accurate position tracking
- 10+ insta snapshot tests pass covering keywords, operators, numbers, identifiers, strings, comments
- All multi-character operators use longest-match-first disambiguation
- Number literals correctly distinguish Int vs Float
- `cargo test` passes cleanly
</success_criteria>

<output>
After completion, create `.planning/phases/01-project-foundation-lexer/01-02-SUMMARY.md`
</output>
