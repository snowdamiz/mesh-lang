---
phase: 01-project-foundation-lexer
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - crates/snow-lexer/src/lib.rs
  - crates/snow-lexer/src/cursor.rs
  - crates/snow-lexer/tests/lexer_tests.rs
  - tests/fixtures/interpolation.snow
  - tests/fixtures/strings.snow
  - tests/fixtures/comments.snow
  - tests/fixtures/error_recovery.snow
  - tests/fixtures/newlines.snow
  - tests/fixtures/full_program.snow
autonomous: true

must_haves:
  truths:
    - "String interpolation `${expr}` emits StringStart, StringContent, InterpolationStart, (expression tokens), InterpolationEnd, StringContent, StringEnd sequence"
    - "Nested interpolation `${a + ${b}}` is NOT supported but `${outer(inner)}` with braces inside interpolation tracks brace depth correctly"
    - "Block comments `#= ... =#` nest correctly: `#= outer #= inner =# still comment =#` is one comment"
    - "Newlines are emitted as Newline tokens (statement terminator handling deferred to parser)"
    - "Error recovery: invalid characters produce Error tokens and lexing continues past them"
    - "Triple-quoted strings with interpolation work correctly"
    - "Lexer can tokenize a complete Snow program with all features and produce correct token stream"
  artifacts:
    - path: "crates/snow-lexer/src/lib.rs"
      provides: "Complete lexer with state stack for interpolation, block comments, newline handling"
      contains: "enum LexerState"
    - path: "crates/snow-lexer/tests/lexer_tests.rs"
      provides: "Comprehensive snapshot tests covering all edge cases"
      min_lines: 100
    - path: "tests/fixtures/full_program.snow"
      provides: "Integration test fixture with all Snow syntax features"
      min_lines: 20
  key_links:
    - from: "crates/snow-lexer/src/lib.rs"
      to: "crates/snow-common/src/token.rs"
      via: "Emits InterpolationStart/InterpolationEnd/StringContent tokens"
      pattern: "InterpolationStart|InterpolationEnd|StringContent"
    - from: "crates/snow-lexer/src/lib.rs"
      to: "crates/snow-common/src/error.rs"
      via: "Creates LexError for error recovery cases"
      pattern: "LexError"
---

<objective>
Complete the lexer with string interpolation (state-stack based), nestable block comments, newline token emission, and comprehensive error recovery. Add exhaustive snapshot tests covering all edge cases, including a full Snow program integration test.

Purpose: This plan handles the lexer's complex features -- string interpolation requires a state machine, block comments need nesting depth tracking, and error recovery must be robust. These features are what separate a toy lexer from a production-quality one. After this plan, the lexer is complete and ready for the parser in Phase 2.

Output: A complete, production-quality lexer that tokenizes all Snow syntax. Comprehensive insta snapshot test suite proving correctness for interpolation, block comments, newlines, error recovery, and a full program. Phase 1 lexer work is done.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-project-foundation-lexer/01-CONTEXT.md
@.planning/phases/01-project-foundation-lexer/01-RESEARCH.md
@.planning/phases/01-project-foundation-lexer/01-01-SUMMARY.md
@.planning/phases/01-project-foundation-lexer/01-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add state stack for string interpolation and nestable block comments</name>
  <files>
    crates/snow-lexer/src/lib.rs
    crates/snow-lexer/src/cursor.rs
  </files>
  <action>
    Add a state stack to the Lexer for tracking nested lexing contexts. This is the core complexity of the lexer.

    **Add LexerState enum to lib.rs:**
    ```rust
    #[derive(Debug, Clone, PartialEq)]
    enum LexerState {
        Normal,
        InString { triple: bool },       // inside a string literal
        InInterpolation { brace_depth: u32 },  // inside ${...}
    }
    ```

    **Update Lexer struct:**
    ```rust
    pub struct Lexer<'src> {
        cursor: Cursor<'src>,
        source: &'src str,
        state_stack: Vec<LexerState>,  // NEW: state stack
    }
    ```

    Initialize with `state_stack: vec![LexerState::Normal]`.

    **Refactor the main `next_token` to be state-aware:**

    Check `self.state_stack.last()` to determine current mode:

    1. **`LexerState::Normal`** -- current behavior (tokenize normally). No changes needed to existing logic except:
       - When encountering `"`: push `InString { triple: false }` (or `InString { triple: true }` for `"""`) and emit `StringStart`
       - When encountering `#=`: implement nestable block comments (see below)
       - Emit `Newline` token when encountering `\n` instead of skipping it. Skip `\r` (carriage return) silently. If `\r\n`, emit single `Newline`.

    2. **`LexerState::InString { triple }`** -- inside a string, scanning content:
       - Scan characters building string content until one of:
         a. `${` found -> emit `StringContent` (if any content accumulated), emit `InterpolationStart`, push `InInterpolation { brace_depth: 0 }` onto state stack
         b. Closing delimiter found (`"` for single, `"""` for triple) -> emit `StringContent` (if any), emit `StringEnd`, pop `InString` from stack
         c. Escape sequence (`\n`, `\t`, `\\`, `\"`, `\r`, `\0`, `\$`) -> include in string content (lexer does NOT process escapes, that's for later phases; just ensure `\"` doesn't end the string and `\$` doesn't start interpolation)
         d. EOF -> emit `Error` token (unterminated string), pop state
         e. For triple-quoted strings: newlines are part of the content (don't end the string)

       Emit `StringContent` tokens for the text between delimiters/interpolations. If there's no content between two interpolations (`"${a}${b}"`), don't emit an empty StringContent.

    3. **`LexerState::InInterpolation { brace_depth }`** -- inside `${...}`:
       - Tokenize NORMALLY (same as Normal state) but track brace depth:
         - `{` -> increment brace_depth (so nested braces in expressions work: `${map[key]}`)
         - `}` -> if brace_depth == 0: emit `InterpolationEnd`, pop `InInterpolation`, return to `InString` state; if brace_depth > 0: decrement and emit `RBrace`
       - This allows arbitrary expressions inside `${}` including function calls with braces, map literals, etc.

    **Nestable block comments (`#= ... =#`):**

    When in Normal state and encountering `#` followed by `=`:
    - Start a block comment scan
    - Track nesting depth (start at 1)
    - Scan forward: `#=` increments depth, `=#` decrements depth
    - When depth reaches 0: block comment is complete
    - Emit `Comment` token spanning the entire block comment (or skip it -- per discretion, emit it for tooling support)
    - If EOF before depth reaches 0: emit `Error` token (unterminated block comment)

    **Newline handling:**

    In Normal state, when encountering `\n`:
    - Emit a `Newline` token
    - Skip consecutive newlines (emit only one Newline for a sequence of blank lines -- or emit each one; per discretion, emit each one and let the parser handle deduplication)
    - `\r\n` should be treated as a single newline
    - `\r` alone (old Mac line endings) should also be treated as a newline

    NOTE: The decision about WHICH newlines are significant (statement terminators vs ignored) is a PARSER concern per the research document's Pattern 3. The lexer just emits all Newline tokens. The parser will filter based on context (after closing delimiters, operators, etc.).
  </action>
  <verify>
    Run `cargo build -p snow-lexer` -- must compile with new state stack.
    Write a quick inline test: tokenize `"hello ${name} world"` and verify it produces: StringStart, StringContent("hello "), InterpolationStart, Ident(name), InterpolationEnd, StringContent(" world"), StringEnd.
    Write a quick inline test: tokenize `#= outer #= inner =# still =#` and verify it produces a single Comment token.
    Run `cargo test -p snow-lexer` -- existing snapshot tests should still pass (update snapshots if Newline tokens now appear where whitespace was previously skipped -- run `cargo insta test --accept -p snow-lexer`).
  </verify>
  <done>
    Lexer has a state stack with Normal, InString, and InInterpolation states. String interpolation `${expr}` correctly emits StringStart/StringContent/InterpolationStart/.../InterpolationEnd/StringContent/StringEnd token sequences. Brace depth tracking allows nested braces inside interpolation expressions. Block comments `#= =#` nest correctly with depth tracking. Newlines emit Newline tokens. All existing tests updated and passing.
  </done>
</task>

<task type="auto">
  <name>Task 2: Comprehensive snapshot tests for interpolation, block comments, newlines, error recovery, and full program</name>
  <files>
    crates/snow-lexer/tests/lexer_tests.rs
    tests/fixtures/interpolation.snow
    tests/fixtures/strings.snow
    tests/fixtures/comments.snow
    tests/fixtures/error_recovery.snow
    tests/fixtures/newlines.snow
    tests/fixtures/full_program.snow
  </files>
  <action>
    Create test fixtures and snapshot tests for all remaining token categories and edge cases.

    **`tests/fixtures/strings.snow`:**
    ```
    "simple string"
    "with \"escape\" sequences\n\t"
    "empty: "
    ""
    ```
    Test basic strings with escapes and empty strings.

    **`tests/fixtures/interpolation.snow`:**
    ```
    "hello ${name}"
    "sum is ${a + b}"
    "${x}"
    "no interpolation"
    "${greeting}, ${name}!"
    "nested braces ${map[key]}"
    ```
    Test various interpolation patterns including adjacent interpolations, expressions with operators, and braces inside interpolation.

    **`tests/fixtures/comments.snow`:**
    ```
    # line comment
    ## doc comment
    ##! module doc comment
    #= block comment =#
    #= nested #= inner =# outer =#
    x = 1 # inline comment
    ```

    **`tests/fixtures/newlines.snow`:**
    ```
    let x = 1
    let y = 2
    let z = x + y
    ```
    Test that newlines appear as Newline tokens between statements.

    **`tests/fixtures/error_recovery.snow`:**
    ```
    let x = @
    let y = 42
    "unterminated string
    let z = 100
    ```
    Test that `@` produces an Error token but lexing continues. Test that unterminated string produces Error and lexing continues on next line.

    **`tests/fixtures/full_program.snow`** -- A realistic Snow program exercising all token categories:
    ```
    ##! A sample Snow module

    module Counter do
      ## Create a new counter
      struct State do
        count :: Int
      end

      pub fn new() -> State do
        State { count: 0 }
      end

      pub fn increment(state) do
        let new_count = state.count + 1
        %State{ state | count: new_count }
      end

      pub fn display(state) do
        "Count is ${state.count}"
      end
    end

    # Main entry point
    let counter = Counter.new()
    let updated = counter |> Counter.increment() |> Counter.increment()
    let message = Counter.display(updated)
    ```

    **Tests to add to `crates/snow-lexer/tests/lexer_tests.rs`:**

    11. `test_simple_string_escapes` -- tokenize `strings.snow`, snapshot.
    12. `test_string_interpolation` -- tokenize `interpolation.snow`, snapshot. Verify StringStart/StringContent/InterpolationStart/.../InterpolationEnd/StringContent/StringEnd sequences.
    13. `test_adjacent_interpolations` -- inline `"${a}${b}"`, verify no empty StringContent between them.
    14. `test_interpolation_with_braces` -- inline `"${map[key]}"` or similar with braces inside interpolation.
    15. `test_triple_quoted_string` -- inline multiline triple-quoted string, verify content includes newlines.
    16. `test_triple_quoted_interpolation` -- inline triple-quoted string with `${expr}`, verify interpolation works in triple-quoted context.
    17. `test_comments` -- tokenize `comments.snow`, snapshot.
    18. `test_nested_block_comment` -- inline `#= outer #= inner =# outer =#`, verify single Comment token.
    19. `test_newlines` -- tokenize `newlines.snow`, snapshot. Verify Newline tokens appear.
    20. `test_error_recovery` -- tokenize `error_recovery.snow`, snapshot. Verify Error tokens present AND subsequent valid tokens are still correct.
    21. `test_full_program` -- tokenize `full_program.snow`, snapshot. This is the integration test proving the lexer handles a complete Snow program.
    22. `test_empty_input` -- inline `""` (empty source), verify only Eof token.
    23. `test_whitespace_only` -- inline `"   \t  "`, verify appropriate handling.
    24. `test_span_accuracy_interpolation` -- tokenize `"hello ${name}"`, verify each token's span points to the correct byte range in source.
    25. `test_unterminated_block_comment` -- inline `#= no close`, verify Error token.

    Run `cargo insta test --accept -p snow-lexer` to generate and accept all new snapshots.
    Then run `cargo test -p snow-lexer` to confirm everything passes.

    IMPORTANT: If any test reveals a lexer bug, fix the lexer in lib.rs/cursor.rs. The tests define correctness. Common issues:
    - Off-by-one in spans around interpolation boundaries
    - Empty StringContent tokens being emitted (should be suppressed)
    - Block comment nesting depth off by one
    - Triple-quoted string not consuming all three closing quotes
    - Escape sequences before `$` preventing interpolation detection
  </action>
  <verify>
    Run `cargo test -p snow-lexer` -- all 25+ tests pass.
    Run `cargo insta test -p snow-lexer` -- no pending snapshots.
    Run `cargo test` from workspace root -- all tests pass across all crates.
    Count snapshot files in `crates/snow-lexer/tests/snapshots/` -- should be 20+ files.
    Inspect the `full_program` snapshot to verify it produces a coherent token stream for a realistic Snow program.
  </verify>
  <done>
    25+ snapshot tests pass covering: simple strings, escape sequences, string interpolation (simple, adjacent, with braces, triple-quoted), block comments (simple, nested, unterminated), newlines, error recovery (invalid chars, unterminated strings), a full Snow program, edge cases (empty input, whitespace-only, span accuracy). The lexer is complete and production-quality. All snapshot files provide human-readable YAML of token streams. `cargo test` passes across entire workspace.
  </done>
</task>

</tasks>

<verification>
1. `cargo test` passes across entire workspace with 25+ lexer tests
2. String interpolation produces correct token sequences (verified by snapshots)
3. Nested block comments `#= #= =# =#` handled correctly
4. Newline tokens emitted (parser will decide which are significant)
5. Error recovery works: invalid chars -> Error token -> lexing continues
6. Unterminated strings and block comments -> Error token -> lexing continues
7. Full program fixture tokenizes into a complete, correct token stream
8. All span byte offsets are accurate (verified by span accuracy test)
9. No pending insta snapshots
</verification>

<success_criteria>
- Lexer handles all Snow syntax: keywords, operators, numbers, identifiers, strings (single/triple-quoted), interpolation, comments (line/doc/module-doc/block), newlines
- String interpolation uses state stack with brace depth tracking
- Block comments nest correctly with depth counter
- Error recovery produces Error tokens and continues lexing
- 25+ snapshot tests cover all token categories and edge cases
- Full program integration test proves end-to-end correctness
- `cargo test` passes cleanly from workspace root
- Phase 1 lexer is complete and ready for Phase 2 parser
</success_criteria>

<output>
After completion, create `.planning/phases/01-project-foundation-lexer/01-03-SUMMARY.md`
</output>
