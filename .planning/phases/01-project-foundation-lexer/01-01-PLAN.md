---
phase: 01-project-foundation-lexer
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Cargo.toml
  - crates/snow-common/Cargo.toml
  - crates/snow-common/src/lib.rs
  - crates/snow-common/src/span.rs
  - crates/snow-common/src/token.rs
  - crates/snow-common/src/error.rs
  - crates/snow-lexer/Cargo.toml
  - crates/snow-lexer/src/lib.rs
  - crates/snowc/Cargo.toml
  - crates/snowc/src/main.rs
  - .cargo/config.toml
  - .gitignore
autonomous: true

must_haves:
  truths:
    - "`cargo build` succeeds on the workspace with LLVM 18 linked"
    - "`cargo test` runs and passes (even if no lexer tests yet)"
    - "TokenKind enum covers all Snow syntax (keywords, operators, literals, identifiers, comments, interpolation markers)"
    - "Span type uses byte offsets (u32) with on-demand line/column computation via LineIndex"
  artifacts:
    - path: "Cargo.toml"
      provides: "Workspace root defining all member crates"
      contains: "members"
    - path: "crates/snow-common/src/token.rs"
      provides: "TokenKind enum with all Snow token variants"
      contains: "pub enum TokenKind"
    - path: "crates/snow-common/src/span.rs"
      provides: "Span struct and LineIndex for byte-offset to line/column conversion"
      contains: "pub struct Span"
    - path: "crates/snow-common/src/error.rs"
      provides: "LexError type for error recovery"
      contains: "pub enum LexErrorKind"
    - path: "crates/snow-lexer/src/lib.rs"
      provides: "Lexer module placeholder (public API surface)"
      contains: "pub struct Lexer"
    - path: "crates/snowc/src/main.rs"
      provides: "Binary entry point stub"
      contains: "fn main"
  key_links:
    - from: "crates/snow-lexer/Cargo.toml"
      to: "crates/snow-common"
      via: "path dependency"
      pattern: 'snow-common.*path'
    - from: "crates/snowc/Cargo.toml"
      to: "crates/snow-lexer"
      via: "path dependency"
      pattern: 'snow-lexer.*path'
---

<objective>
Set up the Rust workspace with multi-crate structure, LLVM 18 build configuration, shared type definitions (TokenKind, Span, LexError), and test infrastructure (insta). This plan creates every file that Plan 02 needs to start writing the lexer.

Purpose: The workspace and shared types are the foundation for the entire compiler. TokenKind must be defined upfront and completely because later plans implement lexing against this enum. Getting the type definitions right here avoids refactoring during lexer implementation.

Output: A buildable Cargo workspace with snow-common (types), snow-lexer (stub), snowc (binary stub), complete TokenKind enum, Span type, LexError type, and insta dependency ready for snapshot tests.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-project-foundation-lexer/01-CONTEXT.md
@.planning/phases/01-project-foundation-lexer/01-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Cargo workspace and crate scaffolding with LLVM 18 config</name>
  <files>
    Cargo.toml
    crates/snow-common/Cargo.toml
    crates/snow-common/src/lib.rs
    crates/snow-lexer/Cargo.toml
    crates/snow-lexer/src/lib.rs
    crates/snowc/Cargo.toml
    crates/snowc/src/main.rs
    .cargo/config.toml
    .gitignore
  </files>
  <action>
    Create the Cargo workspace root at `Cargo.toml` with members: `crates/snow-common`, `crates/snow-lexer`, `crates/snowc`.

    Use workspace-level dependency declarations in `[workspace.dependencies]` for shared deps:
    - `insta = { version = "1.46", features = ["yaml"] }` (snapshot testing)

    **Crate: snow-common** (shared types, no LLVM dependency):
    - `Cargo.toml`: No external dependencies. This crate is pure Rust.
    - `src/lib.rs`: Re-export modules: `pub mod span;`, `pub mod token;`, `pub mod error;`

    **Crate: snow-lexer** (tokenization):
    - `Cargo.toml`: Depends on `snow-common = { path = "../snow-common" }`. Dev-dependency: `insta = { workspace = true }`.
    - `src/lib.rs`: Placeholder with `pub struct Lexer;` stub and a comment indicating this is implemented in Plan 02.

    **Crate: snowc** (binary entry point):
    - `Cargo.toml`: Depends on `snow-lexer = { path = "../snow-lexer" }` and `snow-common = { path = "../snow-common" }`.
    - `src/main.rs`: Minimal `fn main() { println!("snowc: Snow compiler"); }` stub.

    **LLVM 18 build configuration** in `.cargo/config.toml`:
    - Set `LLVM_SYS_180_PREFIX` env var pointing to Homebrew LLVM 18 on macOS: `/opt/homebrew/opt/llvm@18` (Apple Silicon) or `/usr/local/opt/llvm@18` (Intel Mac).
    - Add a comment explaining this is needed for future Phase 5 codegen and is NOT required for Phases 1-4.
    - IMPORTANT: Do NOT add llvm-sys or inkwell as dependencies yet. The lexer and parser do not need LLVM. Structure the workspace so `cargo build` works WITHOUT LLVM installed for now. The `.cargo/config.toml` just pre-configures the env var for when LLVM deps are added later.

    **`.gitignore`**: Standard Rust gitignore (`/target`, `Cargo.lock` -- actually include Cargo.lock since this is a binary project, so just `/target`). Also ignore insta `.snap.new` pending review files.

    IMPORTANT: Do NOT add `llvm-sys` or `inkwell` as dependencies in any crate yet. Those come in Phase 5. The workspace must build with `cargo build` without LLVM installed.
  </action>
  <verify>
    Run `cargo build` from workspace root -- must succeed with no errors.
    Run `cargo test` -- must succeed (0 tests is fine at this stage).
    Verify `cargo run -p snowc` prints "snowc: Snow compiler".
  </verify>
  <done>
    Cargo workspace builds, tests pass, `snowc` binary runs. Three crates exist: snow-common, snow-lexer, snowc. No LLVM dependency required to build.
  </done>
</task>

<task type="auto">
  <name>Task 2: Define complete TokenKind enum, Span type, and LexError type</name>
  <files>
    crates/snow-common/src/token.rs
    crates/snow-common/src/span.rs
    crates/snow-common/src/error.rs
  </files>
  <action>
    **`crates/snow-common/src/span.rs`** -- Span and LineIndex:

    ```rust
    /// Byte-offset span into source text. Start is inclusive, end is exclusive.
    #[derive(Debug, Clone, Copy, PartialEq, Eq)]
    pub struct Span {
        pub start: u32,
        pub end: u32,
    }
    ```

    Add `Span::new(start: u32, end: u32) -> Span` constructor and `Span::len(&self) -> u32`.

    Add `LineIndex` struct that pre-computes newline byte positions from source text:
    ```rust
    pub struct LineIndex {
        line_starts: Vec<u32>,  // byte offset of each line start
    }
    ```
    With methods:
    - `LineIndex::new(source: &str) -> LineIndex` -- scan for `\n` positions
    - `LineIndex::line_col(&self, offset: u32) -> (u32, u32)` -- returns 1-based (line, column) from byte offset via binary search on `line_starts`

    Implement `Serialize` for `Span` (derive or manual) so insta can snapshot it.

    **`crates/snow-common/src/token.rs`** -- Token and TokenKind:

    ```rust
    #[derive(Debug, Clone, PartialEq)]
    pub struct Token {
        pub kind: TokenKind,
        pub span: Span,
    }
    ```

    Define `TokenKind` enum with ALL Snow token variants. This is the complete vocabulary per CONTEXT.md decisions. Use `#[derive(Debug, Clone, PartialEq, Serialize)]` (serde Serialize for insta snapshots).

    Add `serde = { version = "1", features = ["derive"] }` to snow-common's Cargo.toml dependencies.

    **Keywords (37 total -- per research recommendation):**
    `After`, `Alias`, `And`, `Case`, `Cond`, `Def`, `Do`, `Else`, `End`, `False`, `Fn`, `For`, `If`, `Impl`, `Import`, `In`, `Let`, `Link`, `Match`, `Module`, `Monitor`, `Nil`, `Not`, `Or`, `Pub`, `Receive`, `Return`, `Self_`, `Send`, `Spawn`, `Struct`, `Supervisor`, `Trait`, `Trap`, `True`, `Type`, `When`, `Where`, `With`

    Note: Use `Self_` (or `SelfKw`) for the `self` keyword to avoid Rust keyword conflict.

    **Operators:**
    `Plus` (+), `Minus` (-), `Star` (*), `Slash` (/), `EqEq` (==), `NotEq` (!=), `Lt` (<), `Gt` (>), `LtEq` (<=), `GtEq` (>=), `AmpAmp` (&&), `PipePipe` (||), `Bang` (!), `Pipe` (|>), `DotDot` (..), `Diamond` (<>), `PlusPlus` (++), `Eq` (=), `Arrow` (->), `FatArrow` (=>), `ColonColon` (::), `Percent` (%)

    **Delimiters:**
    `LParen`, `RParen`, `LBracket`, `RBracket`, `LBrace`, `RBrace`

    **Punctuation:**
    `Comma`, `Dot`, `Colon`, `Semicolon`, `Newline` (significant as statement terminator)

    **Literals:**
    `IntLiteral` (distinct from float per research recommendation), `FloatLiteral`, `StringStart` (opening `"` or `"""`), `StringEnd` (closing `"` or `"""`), `StringContent` (text between delimiters/interpolations), `InterpolationStart` (`${`), `InterpolationEnd` (`}` that closes interpolation)

    **Identifiers:**
    `Ident` (regular identifier), `DocComment` (content of `##` comment), `ModuleDocComment` (content of `##!` comment)

    **Special:**
    `Comment` (content of `#` line comment -- may be skipped by parser but useful for tooling), `Eof`, `Error` (invalid input, carries error info for recovery)

    **`crates/snow-common/src/error.rs`** -- LexError:

    ```rust
    #[derive(Debug, Clone, PartialEq, Serialize)]
    pub struct LexError {
        pub kind: LexErrorKind,
        pub span: Span,
    }

    #[derive(Debug, Clone, PartialEq, Serialize)]
    pub enum LexErrorKind {
        UnexpectedCharacter(char),
        UnterminatedString,
        UnterminatedBlockComment,
        UnterminatedInterpolation,
        InvalidEscapeSequence(char),
        InvalidNumberLiteral(String),
        // Add more as needed during lexer implementation
    }
    ```

    Implement `Display` for `LexErrorKind` with human-readable messages.

    Implement `Serialize` (via derive) for Token, TokenKind, and LexError so insta snapshots work.

    Add a helper method `Token::new(kind: TokenKind, start: u32, end: u32) -> Token`.

    Also add a `keyword_from_str(s: &str) -> Option<TokenKind>` function (or method) that maps string slices to keyword TokenKind variants. Use a match statement (not a HashMap -- the compiler optimizes match on strings well). This will be used by the lexer to distinguish keywords from identifiers.
  </action>
  <verify>
    Run `cargo build` -- must succeed with all types defined.
    Run `cargo test` -- must succeed.
    Verify `TokenKind` has variants for all 37 keywords, all operators, all delimiters, all punctuation, all literal types, string interpolation tokens, Ident, Comment, DocComment, ModuleDocComment, Eof, and Error by inspecting the file.
    Verify `keyword_from_str("fn")` returns `Some(TokenKind::Fn)` and `keyword_from_str("foo")` returns `None` (add a quick unit test for this).
  </verify>
  <done>
    Complete TokenKind enum with 80+ variants covering all Snow syntax. Span with LineIndex for byte-to-line/column conversion. LexError with typed error kinds. All types derive Serialize for insta. `keyword_from_str` helper tested. `cargo build && cargo test` passes.
  </done>
</task>

</tasks>

<verification>
1. `cargo build` succeeds from workspace root without LLVM installed
2. `cargo test` passes (including keyword_from_str unit test)
3. `cargo run -p snowc` prints compiler stub message
4. Three crates exist: snow-common, snow-lexer, snowc
5. TokenKind enum has variants for all token types listed in CONTEXT.md
6. Span uses u32 byte offsets, NOT line/column storage
7. LineIndex computes line/column from byte offset on demand
8. All types derive serde::Serialize for insta snapshot compatibility
</verification>

<success_criteria>
- Workspace builds cleanly with `cargo build`
- `cargo test` passes with at least 1 test (keyword_from_str)
- TokenKind covers the complete Snow token vocabulary (37 keywords, 22+ operators, 6 delimiters, 5 punctuation, 7+ literal/string tokens, 4+ special tokens)
- Span type is byte-offset based with LineIndex for on-demand line/column
- All shared types are in snow-common, usable by snow-lexer
</success_criteria>

<output>
After completion, create `.planning/phases/01-project-foundation-lexer/01-01-SUMMARY.md`
</output>
