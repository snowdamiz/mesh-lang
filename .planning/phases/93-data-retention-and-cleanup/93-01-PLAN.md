---
phase: 93-data-retention-and-cleanup
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - mesher/storage/schema.mpl
  - mesher/storage/queries.mpl
  - mesher/services/retention.mpl
autonomous: true

must_haves:
  truths:
    - "Schema includes retention_days (INT, default 90) and sample_rate (REAL, default 1.0) columns on projects table"
    - "Queries exist to delete expired events per-project, enumerate expired partitions, and drop them"
    - "Queries exist to get/update project settings and estimate storage per project"
    - "Retention cleaner actor runs on a daily timer cycle using Timer.sleep + recursive call pattern"
  artifacts:
    - path: "mesher/storage/schema.mpl"
      provides: "ALTER TABLE ADD COLUMN for retention_days and sample_rate"
      contains: "retention_days"
    - path: "mesher/storage/queries.mpl"
      provides: "Retention cleanup, storage estimation, settings CRUD, and sampling queries"
      contains: "delete_expired_events"
    - path: "mesher/services/retention.mpl"
      provides: "retention_cleaner actor with daily Timer.sleep cycle"
      contains: "retention_cleaner"
  key_links:
    - from: "mesher/services/retention.mpl"
      to: "mesher/storage/queries.mpl"
      via: "imports cleanup query functions"
      pattern: "from Storage.Queries import"
    - from: "mesher/storage/schema.mpl"
      to: "projects table"
      via: "ALTER TABLE ADD COLUMN IF NOT EXISTS"
      pattern: "retention_days"
---

<objective>
Add retention schema columns, cleanup/storage/settings/sampling queries, and a daily retention cleaner actor.

Purpose: Provides the data foundation and background cleanup engine for per-project data retention, storage visibility, and event sampling. The schema extension, query functions, and cleanup actor form the backend that Plan 02 will wire into the HTTP API and ingestion pipeline.

Output: Modified schema.mpl with retention columns, extended queries.mpl with 7 new query functions, new services/retention.mpl with the retention_cleaner actor.
</objective>

<execution_context>
@/Users/sn0w/.claude/get-shit-done/workflows/execute-plan.md
@/Users/sn0w/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@mesher/storage/schema.mpl
@mesher/storage/queries.mpl
@mesher/ingestion/pipeline.mpl
</context>

<tasks>

<task type="auto">
  <name>Task 1: Schema extension and retention/storage/settings/sampling queries</name>
  <files>mesher/storage/schema.mpl, mesher/storage/queries.mpl</files>
  <action>
**schema.mpl:** Add two idempotent ALTER TABLE statements after line 21 (after the alert_rules ALTER TABLEs), before the CREATE INDEX statements:

```mesh
Pool.execute(pool, "ALTER TABLE projects ADD COLUMN IF NOT EXISTS retention_days INTEGER NOT NULL DEFAULT 90", [])?
Pool.execute(pool, "ALTER TABLE projects ADD COLUMN IF NOT EXISTS sample_rate REAL NOT NULL DEFAULT 1.0", [])?
```

This follows the existing pattern on lines 20-21 where cooldown_minutes and last_fired_at were added to alert_rules.

**queries.mpl:** Add a new section `# --- Retention and storage queries (Phase 93) ---` at the end of the file. Add these 7 pub functions:

1. `delete_expired_events(pool, project_id, retention_days_str) -> Int!String` -- DELETE FROM events WHERE project_id = $1::uuid AND received_at < now() - ($2 || ' days')::interval. Returns affected row count.

2. `get_expired_partitions(pool, max_days_str) -> List<Map<String, String>>!String` -- Query pg_inherits + pg_class to find events child partitions with names matching `events_YYYYMMDD` where the date is older than max_days. SQL: SELECT c.relname::text AS partition_name FROM pg_inherits i JOIN pg_class c ON c.oid = i.inhrelid JOIN pg_class p ON p.oid = i.inhparent WHERE p.relname = 'events' AND c.relname ~ '^events_[0-9]{8}$' AND to_date(substring(c.relname from '[0-9]{8}$'), 'YYYYMMDD') < (current_date - ($1 || ' days')::interval). Param: [max_days_str].

3. `drop_partition(pool, partition_name) -> Int!String` -- Pool.execute(pool, "DROP TABLE IF EXISTS " <> partition_name, []). The partition_name comes from trusted pg_inherits query, not user input.

4. `get_all_project_retention(pool) -> List<Map<String, String>>!String` -- SELECT id::text, retention_days::text FROM projects. Returns all projects with their retention settings for the cleanup loop.

5. `get_project_storage(pool, project_id) -> List<Map<String, String>>!String` -- SELECT count(*)::text AS event_count, (count(*) * 1024)::text AS estimated_bytes FROM events WHERE project_id = $1::uuid. Uses 1024 byte average row estimate.

6. `update_project_settings(pool, project_id, body) -> Int!String` -- UPDATE projects SET retention_days = COALESCE(($2::jsonb->>'retention_days')::int, retention_days), sample_rate = COALESCE(($2::jsonb->>'sample_rate')::real, sample_rate) WHERE id = $1::uuid. Params: [project_id, body]. Uses SQL-side JSON extraction per decision [91-03].

7. `get_project_settings(pool, project_id) -> List<Map<String, String>>!String` -- SELECT retention_days::text, sample_rate::text FROM projects WHERE id = $1::uuid. Returns settings for display.

8. `check_sample_rate(pool, project_id) -> Bool!String` -- SELECT random() < COALESCE((SELECT sample_rate FROM projects WHERE id = $1::uuid), 1.0) AS keep. Param: [project_id]. Parse result: if List.length(rows) > 0 then Ok(Map.get(List.head(rows), "keep") == "t") else Ok(true).

All functions follow existing query patterns: Pool.query/Pool.execute, explicit ::uuid casts, pub visibility, consistent return types.
  </action>
  <verify>
Both files parse correctly in context of the Mesh module system. Grep for "retention_days" in schema.mpl and "delete_expired_events" in queries.mpl to confirm additions exist. Count total pub fn declarations in queries.mpl to verify 8 new functions added.
  </verify>
  <done>
schema.mpl has two new ALTER TABLE statements for retention_days and sample_rate on projects table. queries.mpl has 8 new pub functions covering deletion, partition management, storage estimation, settings CRUD, and sampling.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create retention cleaner actor</name>
  <files>mesher/services/retention.mpl</files>
  <action>
Create a new file `mesher/services/retention.mpl` with the retention_cleaner actor and its helper functions. This module follows the established actor pattern from pipeline.mpl (spike_checker, alert_evaluator).

The module should:

1. Import from Storage.Queries: `delete_expired_events, get_all_project_retention, get_expired_partitions, drop_partition`

2. Define helper functions (leaf-first for define-before-use):

- `log_cleanup_result(deleted :: Int)` -- println with count of deleted events
- `log_cleanup_error(e :: String)` -- println error message
- `log_partition_drop(name :: String)` -- println partition drop message

- `cleanup_projects_loop(pool, projects, i, total, deleted) -> Int!String` -- Loop through projects list by index. For each project: extract id and retention_days_str from Map row, call delete_expired_events(pool, id, retention_days_str), add result to accumulator. Recursive loop pattern matching evaluate_rules_loop in pipeline.mpl.

- `drop_partitions_loop(pool, partitions, i, total) -> Int!String` -- Loop through expired partitions list. For each: extract partition_name from Map row, call drop_partition(pool, partition_name), log the drop. Recursive loop.

- `run_retention_cleanup(pool) -> Int!String` -- Orchestration function:
  1. Get all project retention settings: get_all_project_retention(pool)?
  2. Run per-project deletion loop: cleanup_projects_loop(pool, projects, 0, List.length(projects), 0)?
  3. Get expired partitions (older than 90 days -- max retention): get_expired_partitions(pool, "90")?
  4. Drop expired partitions: drop_partitions_loop(pool, partitions, 0, List.length(partitions))?
  5. Return total deleted count

3. Define the actor (pub):

```mesh
pub actor retention_cleaner(pool :: PoolHandle) do
  Timer.sleep(86400000)
  let result = run_retention_cleanup(pool)
  case result do
    Ok(n) -> log_cleanup_result(n)
    Err(e) -> log_cleanup_error(e)
  end
  retention_cleaner(pool)
end
```

This follows the exact Timer.sleep + recursive call pattern used by spike_checker, alert_evaluator, and health_checker in pipeline.mpl. 86400000ms = 24 hours.

**Important Mesh patterns to follow:**
- Helper functions defined before the actor (define-before-use)
- Single-expression case arms (extract logic to helper functions per decision [88-02])
- Explicit case matching instead of ? operator in the actor body (the ? operator is used in helper functions, not in the actor body pattern)
- All functions that call Pool.query/execute use -> Result pattern (Int!String or similar)
  </action>
  <verify>
File exists at mesher/services/retention.mpl. Contains `pub actor retention_cleaner`. Contains import from Storage.Queries. Contains `Timer.sleep(86400000)` for 24-hour interval. Contains `run_retention_cleanup` orchestration function.
  </verify>
  <done>
services/retention.mpl exists with retention_cleaner actor that runs daily, iterates all projects to delete expired events per their retention_days setting, then drops any partitions older than 90 days (the maximum retention period).
  </done>
</task>

</tasks>

<verification>
1. schema.mpl contains `retention_days` and `sample_rate` ALTER TABLE statements
2. queries.mpl has all 8 new functions: delete_expired_events, get_expired_partitions, drop_partition, get_all_project_retention, get_project_storage, update_project_settings, get_project_settings, check_sample_rate
3. services/retention.mpl exists with retention_cleaner actor using Timer.sleep(86400000) pattern
4. All imports reference correct module paths (Storage.Queries)
5. No compilation baseline regression (should remain at 7-8 pre-existing errors)
</verification>

<success_criteria>
- Schema migration adds retention_days (INT, default 90) and sample_rate (REAL, default 1.0) to projects table
- 8 new query functions cover all retention, storage, settings, and sampling operations
- Retention cleaner actor follows established Timer.sleep + recursive call pattern with 24-hour interval
- Per-project DELETE + global partition DROP hybrid strategy implemented in queries and orchestration
</success_criteria>

<output>
After completion, create `.planning/phases/93-data-retention-and-cleanup/93-01-SUMMARY.md`
</output>
